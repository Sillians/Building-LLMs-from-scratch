{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51993339",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99082b0d",
   "metadata": {},
   "source": [
    "## The three core components of PyTorch\n",
    "\n",
    "Firstly, PyTorch is a tensor library that extends the concept of array-oriented programming\n",
    "library NumPy with the additional feature of accelerated computation on GPUs, thus\n",
    "providing a seamless switch between CPUs and GPUs.\n",
    "\n",
    "Secondly, PyTorch is an automatic differentiation engine, also known as autograd, which\n",
    "enables the automatic computation of gradients for tensor operations, simplifying\n",
    "backpropagation and model optimization.\n",
    "\n",
    "Finally, PyTorch is a deep learning library, meaning that it offers modular, flexible, and\n",
    "efficient building blocks (including pre-trained models, loss functions, and optimizers) for\n",
    "designing and training a wide range of deep learning models, catering to both researchers\n",
    "and developers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfde479",
   "metadata": {},
   "source": [
    "AI is fundamentally about creating computer systems capable of performing tasks that\n",
    "usually require human intelligence. These tasks include understanding natural language,\n",
    "recognizing patterns, and making decisions. (Despite significant progress, AI is still far from\n",
    "achieving this level of general intelligence.)\n",
    "\n",
    "Machine learning represents a subfield of AI that focuses on developing and improving learning algorithms. The key idea behind machine learning is to enable computers to learn from data and make predictions or decisions without being explicitly programmed to perform the task. This involves developing algorithms that can identify patterns and learn from historical data and improve their performance over time with more data and feedback.\n",
    "\n",
    "Machine learning is also behind technologies like recommendation systems used by online retailers and streaming services, email spam filtering, voice recognition in virtual assistants, and even self-driving cars. The introduction and advancement of machine learning have significantly enhanced AI's capabilities, enabling it to move beyond strict rule-based systems and adapt to new inputs or changing environments.\n",
    "\n",
    "Deep learning is a subcategory of machine learning that focuses on the training and\n",
    "application of deep neural networks. These deep neural networks were originally inspired by\n",
    "how the human brain works, particularly the interconnection between many neurons. The\n",
    "\"deep\" in deep learning refers to the multiple hidden layers of artificial neurons or nodes\n",
    "that allow them to model complex, nonlinear relationships in the data.\n",
    "\n",
    "Unlike traditional machine learning techniques that excel at simple pattern recognition,\n",
    "deep learning is particularly good at handling unstructured data like images, audio, or text,\n",
    "so deep learning is particularly well suited for LLMs.\n",
    "\n",
    "Using a learning algorithm, a model is trained on a training dataset consisting of examples\n",
    "and corresponding labels. In the case of an email spam classifier, for example, the training\n",
    "dataset consists of emails and their spam and not-spam labels that a human identified.\n",
    "Then, the trained model can be used on new observations (new emails) to predict their\n",
    "unknown label (spam or not spam)\n",
    "\n",
    "Of course, we also want to add a model evaluation between the training and inference\n",
    "stages to ensure that the model satisfies our performance criteria before using it in a real-\n",
    "world application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2c3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c0b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd03841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce464367",
   "metadata": {},
   "source": [
    "## Understanding tensors\n",
    "\n",
    "Tensors represent a mathematical concept that generalizes vectors and matrices to potentially higher dimensions. In other words, tensors are mathematical objects that can be characterized by their order (or rank), which provides the number of dimensions. For example, a scalar (just a number) is a tensor of rank 0, a vector is a tensor of rank 1, and a matrix is a tensor of rank 2.\n",
    "\n",
    "From a computational perspective, tensors serve as data containers. For instance, they hold multi-dimensional data, where each dimension represents a different feature. Tensor libraries, such as PyTorch, can create, manipulate, and compute with these multi-dimensional arrays efficiently. In this context, a tensor library functions as an array library.\n",
    "\n",
    "\n",
    "PyTorch tensors are similar to NumPy arrays but have several additional features important for deep learning. For example, PyTorch adds an automatic differentiation engine, simplifying computing gradients. PyTorch tensors also support GPU computations to speed up deep neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1a146",
   "metadata": {},
   "source": [
    "## Scalars, vectors, matrices, and tensors\n",
    "\n",
    "As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar\n",
    "is a `0-dimensional` tensor (for instance, just a number), a vector is a `1-dimensional` tensor,\n",
    "and a matrix is a `2-dimensional` tensor. There is no specific term for higher-dimensional\n",
    "tensors, so we typically refer to a `3-dimensional` tensor as just a 3D tensor, and so forth.\n",
    "\n",
    "We can create objects of PyTorch's Tensor class using the torch.tensor function as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2ae749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c63306f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor0d = torch.tensor(1)\n",
    "print(tensor0d, tensor0d.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3626b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) 1\n"
     ]
    }
   ],
   "source": [
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d, tensor1d.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f46fc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4],\n",
      "        [6, 7]]) 2\n"
     ]
    }
   ],
   "source": [
    "tensor2d = torch.tensor([[3, 4], [6, 7]])\n",
    "print(tensor2d, tensor2d.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb59b42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 6],\n",
      "         [3, 7],\n",
      "         [4, 8]]]) 3\n"
     ]
    }
   ],
   "source": [
    "tensor3d = torch.tensor([[[2, 6], [3, 7], [4, 8]]])\n",
    "print(tensor3d, tensor3d.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8a7d9",
   "metadata": {},
   "source": [
    "## Tensor data types\n",
    "\n",
    "PyTorch adopts the default 64-bit integer data type from Python. We can access the data type of a\n",
    "tensor via the `.dtype` attribute of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48461bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfddd8",
   "metadata": {},
   "source": [
    "If we create tensors from Python floats, PyTorch creates tensors with a 32-bit precision by\n",
    "default, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e41693f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = torch.tensor([1.0, 2.3, 4.6])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce77f6",
   "metadata": {},
   "source": [
    "This choice is primarily due to the balance between precision and computational efficiency.\n",
    "A 32-bit floating point number offers sufficient precision for most deep learning tasks, while\n",
    "consuming less memory and computational resources than a 64-bit floating point number.\n",
    "Moreover, GPU architectures are optimized for 32-bit computations, and using this data\n",
    "type can significantly speed up model training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea0b11",
   "metadata": {},
   "source": [
    "Moreover, it is possible to readily change the precision using a tensor's `.to` method. The following code demonstrates this by changing a `64-bit` integer tensor into a `32-bit` float tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c4055a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "vectofloat = tensor1d.to(torch.float32)\n",
    "print(vectofloat, vectofloat.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f0d20",
   "metadata": {},
   "source": [
    "## Common PyTorch tensor operations\n",
    "\n",
    "Some of the most essential PyTorch tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3721d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6],\n",
      "        [ 4,  8],\n",
      "        [ 5, 10]]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# We already introduced the torch.tensor() function to create new tensors.\n",
    "\n",
    "new_tensor = torch.tensor([[3, 6], [4, 8], [5, 10]])\n",
    "print(new_tensor, new_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a39c64c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In addition, the .shape attribute allows us to access the shape of a tensor:\n",
    "\n",
    "new_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e744f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6,  4],\n",
      "        [ 8,  5, 10]])\n"
     ]
    }
   ],
   "source": [
    "# As you can see above, .shape returns [3, 2], which means that the tensor has 3 rows and 2 columns. \n",
    "# To reshape the tensor into a 2 by 3 tensor, we can use the .reshape method:\n",
    "\n",
    "print(new_tensor.reshape(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00791d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6,  4],\n",
      "        [ 8,  5, 10]])\n"
     ]
    }
   ],
   "source": [
    "# However, note that the more common command for reshaping tensors in PyTorch is .view():\n",
    "\n",
    "print(new_tensor.view(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608d574",
   "metadata": {},
   "source": [
    "Similar to `.reshape` and `.view`, there are several cases where PyTorch offers multiple syntax options for executing the same computation.\n",
    "This is because PyTorch initially followed the original `Lua Torch` syntax convention but then also added syntax to make it more similar to NumPy upon popular request.\n",
    "\n",
    "Next, we can use `.T` to transpose a tensor, which means flipping it across its diagonal. Note that this is similar from reshaping a tensor as you can see based on the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6abf5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  4,  5],\n",
      "        [ 6,  8, 10]])\n"
     ]
    }
   ],
   "source": [
    "print(new_tensor.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25d59e",
   "metadata": {},
   "source": [
    "Lastly, the common way to multiply two matrices in PyTorch is the `.matmul` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f14bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33, 40],\n",
      "        [60, 73]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.matmul(tensor2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98248c9",
   "metadata": {},
   "source": [
    "However, we can also adopt the `@` operator, which accomplishes the same thing more compactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25d28a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33, 40],\n",
      "        [60, 73]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d@tensor2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e4a1c",
   "metadata": {},
   "source": [
    "## Seeing models as computation graphs\n",
    "\n",
    "PyTorch's autograd system provides functions to compute gradients in dynamic computational graphs automatically. \n",
    "\n",
    "A computational graph (or computation graph in short) is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network\n",
    "\n",
    "Let's look at a concrete example to illustrate the concept of a computation graph. The following code implements the forward pass (prediction step) of a simple logistic regression classifier, which can be seen as a single-layer neural network, returning a score between `0` and `1` that is compared to the true class label `(0 or 1)` when computing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2aa5a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9183])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2])\n",
    "b = torch.Tensor([0.0])\n",
    "z = x1 * w1 + b\n",
    "\n",
    "y_pred = torch.sigmoid(z)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fec74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loss\n",
    "loss = F.binary_cross_entropy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52bcbdbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0852)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb507a8",
   "metadata": {},
   "source": [
    "![Alt text](../assests/computational-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd924e5",
   "metadata": {},
   "source": [
    "The image above illustrates A logistic regression forward pass as a computation graph. The input feature `x1` is multiplied by a model weight `w1` and passed through an activation function `Ïƒ` after adding the bias. The loss is computed by comparing the model output `a` with a given label `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799a495",
   "metadata": {},
   "source": [
    "In fact, PyTorch builds such a computation graph in the background, and we can use this to\n",
    "calculate gradients of a loss function with respect to the model parameters (here w1 and b)\n",
    "to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658be28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a53b9a5",
   "metadata": {},
   "source": [
    "## Automatic differentiation made easy\n",
    "\n",
    "If we carry out computations in PyTorch, it will build such a graph internally by default if one of its terminal nodes has the `requires_grad` attribute set to True. This is useful if we want to compute `gradients`. Gradients are required when training neural networks via the popular backpropagation algorithm, which can be thought of as an implementation of the `chain rule` from calculus for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deef4bd",
   "metadata": {},
   "source": [
    "![Alt text](../assests/auto-diff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b38d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d44e80b",
   "metadata": {},
   "source": [
    "# PARTIAL DERIVATIVES AND GRADIENTS\n",
    "\n",
    "`Figure A.8` shows partial derivatives, which measure the rate at which a function changes with respect to one of its variables. A gradient is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input.\n",
    "\n",
    "If you are not familiar or don't remember the `partial derivatives`, `gradients`, or the `chain rule` from calculus, don't worry. On a high level, all you need to know for this book is that the chain rule is a way to compute gradients of a loss function with respect to the model's parameters in a computation graph. This provides the information needed to update each parameter in a way that minimizes the loss function, which serves as a proxy for measuring the model's performance, using a method such as gradient descent.\n",
    "\n",
    "the automatic differentiation `(autograd)` engine? By tracking every operation performed on tensors, PyTorch's `autograd` engine constructs a computational graph in the background. Then, calling the `grad` function, we can compute the gradient of the loss with respect to model parameter `w1` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77752504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "z = x1 * w1 + b\n",
    "\n",
    "y_pred = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(y_pred, y)\n",
    "\n",
    "grad_l_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_l_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d870f",
   "metadata": {},
   "source": [
    "By default, PyTorch destroys the computation graph after calculating the gradients to free memory. However, since we are going to reuse this computation graph shortly, we set retain_graph=True so that it stays in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e89caddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0817]),) (tensor([-0.0898]),)\n"
     ]
    }
   ],
   "source": [
    "print(grad_l_b, grad_l_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81e98c",
   "metadata": {},
   "source": [
    "Above, we have been using the grad function `\"manually,\"` which can be useful for experimentation, debugging, and demonstrating concepts. But in practice, PyTorch provides even more high-level tools to automate this process. For instance, we can call `.backward` on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph, which will be stored via the tensors' `.grad` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c99e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5435b0",
   "metadata": {},
   "source": [
    "## Implementing multilayer neural networks\n",
    "\n",
    "In the previous sections, we covered PyTorch's tensor and autograd components. This section focuses on PyTorch as a library for implementing `deep neural networks`. To provide a concrete example, we focus on a `multilayer perceptron`, which is a fully\n",
    "connected neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a1b3a",
   "metadata": {},
   "source": [
    "![Alt text](../assests/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b328b",
   "metadata": {},
   "source": [
    "When implementing a neural network in PyTorch, we typically subclass the `torch.nn.Module` class to define our own custom network architecture. This Module base class provides a lot of functionality, making it easier to build and train models. For instance, it allows us to encapsulate layers and operations and keep track of the model's parameters.\n",
    "\n",
    "Within this subclass, we define the network layers in the `__init__` constructor and specify how they interact in the `forward method`. The forward method describes how the input data passes through the network and comes together as a computation graph.\n",
    "\n",
    "In contrast, the `backward` method, which we typically do not need to implement ourselves, is used during training to compute `gradients` of the `loss function` with respect to the `model parameters`. \n",
    "\n",
    "The following code implements a classic `multilayer perceptron` with two hidden layers to illustrate a typical usage of the Module class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a40f1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(\n",
    "            \n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682997cb",
   "metadata": {},
   "source": [
    "- It's useful to code the number of inputs and outputs as variables to reuse the same code for datasets with different numbers of features and classes.\n",
    "  \n",
    "- The Linear layer takes the number of input and output nodes as arguments.\n",
    "  \n",
    "- Nonlinear activation functions are placed between the hidden layers.\n",
    "  \n",
    "- The number of output nodes of one hidden layer has to match the number of inputs of the next layer.\n",
    "  \n",
    "- The outputs of the last layer are called logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e5055ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a new neural network object as follows:\n",
    "model = NeuralNetwork(50, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa150745",
   "metadata": {},
   "source": [
    "But before using this new model object, it is often useful to call print on the model to see a summary of its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17c17476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e77c0",
   "metadata": {},
   "source": [
    "Note that we used the `Sequential` class when we implemented the `NeuralNetwork` class. Using Sequential is not required, but it can make our life easier if we have a series of layers that we want to execute in a specific order, as is the case here. This way, after instantiating `self.layers = Sequential(...)` in the `__init__` constructor, we just have to call the `self.layers` instead of calling each layer individually in the `NeuralNetwork's` forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b3e7e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "# Next, let's check the total number of trainable parameters of this model:\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable model parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f3b5f",
   "metadata": {},
   "source": [
    "Note that each parameter for which `requires_grad=True` counts as a trainable parameter and will be updated during training.\n",
    "\n",
    "In the case of our `neural network model` with the two hidden layers above, these trainable parameters are contained in the `torch.nn.Linear` layers. A linear layer multiplies the `inputs with a weight matrix and adds a bias vector`. This is sometimes also referred to\n",
    "as a `feedforward` or `fully connected layer`.\n",
    "\n",
    "Based on the `print(model)` call we executed above, we can see that the first Linear layer is at index position `0` in the layers attribute. We can access the corresponding weight parameter matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6ee8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1002,  0.0516,  0.0463,  ..., -0.1280, -0.1195,  0.0278],\n",
      "        [ 0.1319,  0.0549,  0.1327,  ...,  0.0467, -0.0404,  0.1108],\n",
      "        [-0.0768,  0.0059,  0.0921,  ..., -0.0913, -0.1009,  0.0777],\n",
      "        ...,\n",
      "        [-0.1242,  0.0902,  0.0313,  ...,  0.0348, -0.0284,  0.0580],\n",
      "        [-0.0845,  0.1084, -0.0515,  ...,  0.0223,  0.0502,  0.0350],\n",
      "        [ 0.0059, -0.0862, -0.0601,  ...,  0.0730, -0.1253,  0.0554]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfdf19bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0706, -0.1613, -0.0382, -0.0338,  0.0408,  0.0283, -0.1800,  0.0613,\n",
      "          0.0337, -0.1748,  0.0415, -0.1285,  0.0826,  0.1203,  0.1686, -0.0867,\n",
      "         -0.1456, -0.1658, -0.0388,  0.1806, -0.1766,  0.1511, -0.0600,  0.1367,\n",
      "         -0.0169,  0.1419,  0.1603,  0.0325,  0.1771,  0.1390],\n",
      "        [-0.0345, -0.0200, -0.1431,  0.1416, -0.1668, -0.1217,  0.0388,  0.0628,\n",
      "         -0.1759,  0.0356, -0.1225, -0.1818,  0.0592,  0.0677,  0.0149,  0.0779,\n",
      "          0.0678, -0.1519, -0.0082, -0.1400,  0.1643,  0.0040,  0.0731, -0.0616,\n",
      "         -0.0788,  0.1778, -0.0299, -0.0703, -0.1403,  0.0580],\n",
      "        [-0.1791, -0.0134, -0.1733,  0.1272, -0.0072,  0.1291, -0.0544, -0.0058,\n",
      "          0.1186,  0.1017, -0.1676,  0.1193,  0.0153,  0.0970,  0.0720,  0.1543,\n",
      "         -0.0394,  0.0698, -0.0334, -0.0784, -0.0734,  0.0118,  0.0591, -0.0523,\n",
      "          0.0371,  0.0506,  0.1024,  0.1455, -0.0425, -0.0215],\n",
      "        [ 0.1729,  0.0291, -0.0773,  0.0555, -0.0129,  0.0936, -0.0507,  0.1312,\n",
      "         -0.0412,  0.1128,  0.0224,  0.0815,  0.1071, -0.1019,  0.0689,  0.0635,\n",
      "          0.0032, -0.0200,  0.1782,  0.0478, -0.1793,  0.1111,  0.1507, -0.0339,\n",
      "         -0.0842,  0.0776,  0.1028, -0.0634,  0.1683,  0.1774],\n",
      "        [ 0.0897, -0.1527, -0.0023,  0.0621, -0.1655,  0.0062,  0.1813,  0.0050,\n",
      "          0.1479, -0.0681, -0.1116, -0.0995,  0.1503, -0.1600, -0.0795,  0.1131,\n",
      "          0.1508,  0.0512, -0.1590, -0.1477, -0.1628, -0.0971, -0.0545, -0.0701,\n",
      "          0.0511, -0.1470,  0.1398,  0.1482,  0.1093,  0.0295],\n",
      "        [ 0.1034, -0.1324,  0.0522, -0.1763,  0.1302, -0.1013,  0.1153, -0.0675,\n",
      "         -0.0857, -0.1337, -0.1500,  0.0964, -0.1333,  0.0509,  0.0277, -0.0554,\n",
      "         -0.0453,  0.1533, -0.0149, -0.0429,  0.0464, -0.0397,  0.1256,  0.1391,\n",
      "         -0.0057, -0.1631,  0.0756, -0.0831,  0.1691,  0.0215],\n",
      "        [-0.1499,  0.1179, -0.0704,  0.1297,  0.1664,  0.1539, -0.0317,  0.0366,\n",
      "         -0.0618,  0.1341,  0.0178,  0.1610, -0.1077,  0.1518, -0.1720,  0.1321,\n",
      "         -0.0275, -0.0994, -0.0367,  0.0831, -0.0816,  0.0758, -0.0843, -0.1265,\n",
      "          0.0866,  0.0711,  0.0087,  0.1590, -0.0604, -0.0992],\n",
      "        [-0.0320, -0.1323,  0.1372,  0.0776, -0.0064,  0.0733,  0.0792, -0.0549,\n",
      "         -0.1547, -0.0701,  0.1147, -0.0816, -0.1710, -0.1820, -0.0038, -0.0864,\n",
      "          0.1251, -0.0873,  0.0202, -0.0765,  0.0061, -0.0433,  0.1210, -0.1582,\n",
      "          0.0005, -0.0093,  0.1616,  0.1601, -0.0877, -0.0754],\n",
      "        [-0.1749,  0.1736, -0.1189, -0.0411, -0.0259, -0.0722, -0.1471,  0.0117,\n",
      "          0.1427, -0.0150,  0.0765, -0.1705, -0.0619,  0.1796,  0.0292,  0.1224,\n",
      "         -0.0892, -0.1556,  0.0655,  0.0004,  0.1053,  0.0367, -0.1415, -0.0346,\n",
      "          0.1376,  0.1772,  0.0217,  0.1121,  0.1753,  0.0276],\n",
      "        [ 0.1586,  0.0395, -0.0888, -0.0917,  0.0829,  0.1371,  0.0925, -0.1620,\n",
      "          0.0481,  0.0020,  0.0840, -0.0824, -0.1493, -0.0726,  0.1037,  0.0858,\n",
      "          0.0293, -0.0568,  0.1124,  0.1439, -0.1423, -0.0077,  0.1668, -0.0859,\n",
      "          0.1801,  0.1706, -0.1141,  0.1086, -0.0405, -0.1661],\n",
      "        [ 0.1062,  0.0634, -0.0479, -0.0907,  0.0142, -0.1526, -0.0038,  0.0384,\n",
      "         -0.0853, -0.1645, -0.0260,  0.0136,  0.1028, -0.0040,  0.0849,  0.0036,\n",
      "          0.0248,  0.1068, -0.0823, -0.0902,  0.0464, -0.0450, -0.1027,  0.0260,\n",
      "         -0.1082,  0.0817,  0.0647, -0.1065, -0.0768,  0.0737],\n",
      "        [ 0.0849,  0.1004, -0.0867, -0.1265,  0.0142,  0.1115,  0.1409,  0.1192,\n",
      "         -0.0130, -0.0946, -0.0218,  0.0682, -0.0016, -0.0138, -0.1722, -0.0929,\n",
      "         -0.0353,  0.1061, -0.0998,  0.1393,  0.0696,  0.1035, -0.1821,  0.0017,\n",
      "          0.0841,  0.1540, -0.0246,  0.1079, -0.0918, -0.1635],\n",
      "        [ 0.0844,  0.1117,  0.0687,  0.1545, -0.1452,  0.0173, -0.1318,  0.0059,\n",
      "         -0.0548, -0.1170, -0.0484, -0.0916, -0.1011, -0.1305,  0.0735,  0.0122,\n",
      "          0.1239,  0.0705, -0.0486,  0.1518, -0.0565,  0.1265,  0.0677, -0.0586,\n",
      "         -0.1178, -0.0288, -0.0418,  0.0972, -0.0598, -0.0096],\n",
      "        [-0.1074, -0.0727, -0.1825, -0.0593,  0.0615, -0.1191, -0.1365,  0.0442,\n",
      "         -0.1642,  0.1448, -0.1232,  0.0881, -0.1683,  0.0586,  0.1305,  0.0223,\n",
      "         -0.0460,  0.1042,  0.1286, -0.1255,  0.0698, -0.0119,  0.0351, -0.0505,\n",
      "         -0.1026,  0.0266,  0.1299,  0.1392, -0.1489, -0.0094],\n",
      "        [ 0.1153,  0.0918, -0.1033, -0.0933, -0.0782,  0.1199, -0.0665,  0.0503,\n",
      "         -0.0358, -0.1602,  0.0959, -0.0145,  0.0587,  0.0075,  0.0857, -0.0497,\n",
      "          0.0087, -0.0627,  0.1532,  0.0236, -0.0192, -0.0638, -0.0904,  0.1401,\n",
      "         -0.1233,  0.1114,  0.1468,  0.0225,  0.0831, -0.1760],\n",
      "        [-0.1094, -0.0863,  0.1152,  0.1111,  0.0887, -0.0108, -0.1562,  0.0565,\n",
      "         -0.0607, -0.0111,  0.1472, -0.0508, -0.0889, -0.1634,  0.0611,  0.0017,\n",
      "         -0.1137,  0.1816, -0.0555, -0.0662,  0.0979, -0.1635, -0.0165, -0.1511,\n",
      "         -0.0848, -0.0064, -0.0606,  0.0059, -0.1269,  0.0832],\n",
      "        [ 0.1712,  0.0664, -0.0418,  0.0118, -0.0977, -0.0202, -0.1773,  0.1438,\n",
      "         -0.0387,  0.1552,  0.0739,  0.1730,  0.0973,  0.1673,  0.1545,  0.1702,\n",
      "          0.0314,  0.0194, -0.1786,  0.1618,  0.1092, -0.0990, -0.1792,  0.0671,\n",
      "          0.1215,  0.1289,  0.1711, -0.1027,  0.1145,  0.0810],\n",
      "        [ 0.1690, -0.0695, -0.1611, -0.0463, -0.1221, -0.1716,  0.1026, -0.1476,\n",
      "          0.0275, -0.1396,  0.0055, -0.0885,  0.1732,  0.1753, -0.1320,  0.0500,\n",
      "         -0.0054,  0.0075,  0.1339,  0.1185,  0.0270,  0.0884, -0.0401,  0.1405,\n",
      "         -0.0081, -0.1809,  0.1771,  0.1209, -0.0814, -0.0300],\n",
      "        [-0.1314, -0.0350, -0.0043, -0.1036, -0.1620,  0.0065,  0.0105,  0.1210,\n",
      "         -0.1455,  0.1259,  0.0021, -0.0159,  0.1349,  0.1728, -0.0602,  0.1188,\n",
      "         -0.0630, -0.0133,  0.1785, -0.0422,  0.1782, -0.0338,  0.1489,  0.0531,\n",
      "         -0.0590, -0.0728, -0.0856,  0.1204, -0.0073,  0.1049],\n",
      "        [ 0.1494,  0.1246, -0.0551,  0.1672, -0.0232,  0.1538,  0.0206,  0.0391,\n",
      "         -0.0375, -0.1357,  0.1512,  0.0578,  0.0289, -0.0969,  0.1557, -0.1263,\n",
      "         -0.1417, -0.0407, -0.0715,  0.0860,  0.1567,  0.1127,  0.1509,  0.1391,\n",
      "         -0.0537,  0.0404, -0.1807, -0.1542,  0.0430, -0.0171]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e244105",
   "metadata": {},
   "source": [
    "Since this is a large matrix that is not shown in its entirety, let's use the .shape attribute to show its dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78957549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the weight of the first layer: torch.Size([30, 50])\n",
      "Shape of the weight of the second layer: torch.Size([20, 30])\n"
     ]
    }
   ],
   "source": [
    "first_layer = model.layers[0].weight.shape\n",
    "second_layer = model.layers[2].weight.shape\n",
    "\n",
    "print(f\"Shape of the weight of the first layer:\", first_layer)\n",
    "print(f\"Shape of the weight of the second layer:\", second_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3bade",
   "metadata": {},
   "source": [
    "(Similarly, you could access the bias vector via `model.layers[0].bias`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83f6347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the bias of the first layer: torch.Size([30])\n",
      "Shape of the bias of the second layer: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "first_layer_bias = model.layers[0].bias.shape\n",
    "second_layer_bias = model.layers[2].bias.shape\n",
    "\n",
    "print(f\"Shape of the bias of the first layer:\", first_layer_bias)\n",
    "print(f\"Shape of the bias of the second layer:\", second_layer_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f403a",
   "metadata": {},
   "source": [
    "The weight matrix of the first layer above is a `30x50` matrix, and we can see that the `requires_grad` is set to True, which means its entries are trainable -- this is the default setting for `weights` and `biases` in `torch.nn.Linear`.\n",
    "\n",
    "Note that if you execute the code above on your computer, the numbers in the weight matrix will likely differ from those shown above. This is because the model weights are initialized with small random numbers, which are different each time we instantiate the network. In deep learning, initializing model weights with small random numbers is desired to break symmetry during training -- otherwise, the nodes would be just performing the same operations and updates during backpropagation.\n",
    "\n",
    "However, while we want to keep using small random numbers as initial values for our layer weights, we can make the random number initialization reproducible by `seeding` PyTorch's random number generator via `manual_seed`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6b886cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62632995",
   "metadata": {},
   "source": [
    "Now, after we spent some time inspecting the `NeuraNetwork` instance, let's briefly see how it's used via the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ffc44ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "X = torch.rand((1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fcd7b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740, 0.8665, 0.1366, 0.1025, 0.1841,\n",
       "         0.7264, 0.3153, 0.6871, 0.0756, 0.1966, 0.3164, 0.4017, 0.1186, 0.8274,\n",
       "         0.3821, 0.6605, 0.8536, 0.5932, 0.6367, 0.9826, 0.2745, 0.6584, 0.2775,\n",
       "         0.8573, 0.8993, 0.0390, 0.9268, 0.7388, 0.7179, 0.7058, 0.9156, 0.4340,\n",
       "         0.0772, 0.3565, 0.1479, 0.5331, 0.4066, 0.2318, 0.4545, 0.9737, 0.4606,\n",
       "         0.5159, 0.4220, 0.5786, 0.9455, 0.8057]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae725e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50]), 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13de67b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f8795",
   "metadata": {},
   "source": [
    "In the code above, we generated a single random training example `X` as a toy input (note that our network expects `50-dimensional` feature vectors) and fed it to the model, returning three scores. When we call `model(x)`, it will automatically execute the forward pass of the\n",
    "model.\n",
    "\n",
    "The forward pass refers to calculating output tensors from input tensors. This involves passing the input data through all the neural network layers, starting from the input layer, through hidden layers, and finally to the output layer.\n",
    "\n",
    "These three numbers returned above correspond to a score assigned to each of the three output nodes. Notice that the output tensor also includes a `grad_fn` value.\n",
    "\n",
    "Here, `grad_fn=<AddmmBackward0>` represents the last-used function to compute a variable in the computational graph. In particular, `grad_fn=<AddmmBackward0>` means that the tensor we are inspecting was created via a matrix multiplication and addition operation.\n",
    "PyTorch will use this information when it computes gradients during backpropagation. The `<AddmmBackward0>` part of `grad_fn=<AddmmBackward0>` specifies the operation that was performed. In this case, it is an `Addmm` operation. `Addmm` stands for `matrix multiplication (mm)` followed by an `addition (Add)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b43f20",
   "metadata": {},
   "source": [
    "If we just want to use a network without training or backpropagation, for example, if we use it for prediction after training, constructing this computational graph for backpropagation can be wasteful as it performs unnecessary computations and consumes additional memory. So, when we use a model for inference (for instance, making predictions) rather than training, it is a best practice to use the `torch.no_grad()` context manager, as shown below. This tells PyTorch that it doesn't need to keep track of the gradients, which can result in significant savings in memory and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "31bace6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc33ef",
   "metadata": {},
   "source": [
    "In PyTorch, it's common practice to code models such that they return the outputs of the `last layer (logits)` without passing them to a `nonlinear activation` function. That's because `PyTorch's` commonly used loss functions combine the `softmax (or sigmoid for binary\n",
    "classification)` operation with the `negative log-likelihood loss` in a single class. The reason for this is numerical efficiency and stability. So, if we want to compute `class-membership probabilities` for our predictions, we have to call the `softmax` function explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "257156b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)\n",
    "out.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d50b74",
   "metadata": {},
   "source": [
    "The values can now be interpreted as `class-membership probabilities` that sum up to `1`. The values are roughly equal for this random input, which is expected for a randomly initialized model without training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0120d1",
   "metadata": {},
   "source": [
    "## Setting up efficient data loaders\n",
    "\n",
    "In PyTorch, a `DataLoader` is a `utility class` within the `torch.utils.data` module that facilitates efficient data loading for training and evaluating machine learning models. It acts as an interface between your dataset and the model, abstracting away complexities like batching, shuffling, and parallel data loading.\n",
    "\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "**Wraps a Dataset:** The `DataLoader` takes a `Dataset` object as input. This `Dataset` object can be a built-in PyTorch dataset `(e.g., MNIST, CIFAR-10)` or a custom dataset you define by inheriting from `torch.utils.data.Dataset`.\n",
    "\n",
    "\n",
    "**Batching:** It groups individual data samples from the dataset into mini-batches, which are then fed to the model for training or inference. This is crucial for efficient training, especially with large datasets, as it allows for `parallel processing` and better utilization of hardware resources.\n",
    "\n",
    "\n",
    "**Shuffling:** It can shuffle the data samples at the beginning of each epoch (or before training) to ensure that the model does not learn patterns based on the order of the data.\n",
    "\n",
    "\n",
    "**Parallel Loading (Optional):** It can use multiple worker processes (`num_workers` parameter) to load data in parallel, which can significantly speed up the data loading process, preventing the GPU from waiting for data.\n",
    "\n",
    "\n",
    "**Iteration:** The `DataLoader` makes the dataset iterable, allowing you to easily loop through the data in batches during your training or evaluation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a688c9a",
   "metadata": {},
   "source": [
    "![Alt text](../assests/dataloader.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2efdea",
   "metadata": {},
   "source": [
    "Following the `illustration` in `figure A.10`, in this section, we will implement a custom Dataset\n",
    "class that we will use to create a training and a test dataset that we'll then use to create\n",
    "the data loaders.\n",
    "\n",
    "Let's start by creating a simple toy dataset of five training examples with two features each. Accompanying the training examples, we also create a tensor containing the corresponding class labels: three examples belong to `class 0`, and two examples belong to `class 1`. In addition, we also make a `test set` consisting of two entries. \n",
    "\n",
    "The code to create this dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5d55c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "72bbb1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.2000,  3.1000],\n",
       "         [-0.9000,  2.9000],\n",
       "         [-0.5000,  2.6000],\n",
       "         [ 2.3000, -1.1000],\n",
       "         [ 2.7000, -1.5000]]),\n",
       " tensor([[-0.8000,  2.8000],\n",
       "         [ 2.6000, -1.6000]]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4b12efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([2, 2]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c3834d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533e453",
   "metadata": {},
   "source": [
    "**CLASS LABEL NUMBERING** PyTorch requires that class labels start with `label 0`, and the largest class label value should not exceed the number of output nodes minus 1 (since Python index counting\n",
    "starts at 0. So, if we have class labels `0, 1, 2, 3, and 4`, the neural network output layer should consist of 5 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a443c",
   "metadata": {},
   "source": [
    "Next, we create a custom dataset class, `ToyDataset`, by subclassing from PyTorch's Dataset parent class, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e044bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b79f94",
   "metadata": {},
   "source": [
    "This custom `ToyDataset` class's purpose is to use it to instantiate a PyTorch `DataLoader`.\n",
    "But before we get to this step, let's briefly go over the general structure of the `ToyDataset`\n",
    "code.\n",
    "\n",
    "In PyTorch, the three main components of a custom Dataset class are the `__init__`\n",
    "constructor, the `__getitem__` method, and the `__len__` method, as shown in code listing above.\n",
    "\n",
    "In the `__init__` method, we set up attributes that we can access later in the `__getitem__` and `__len__` methods. This could be file paths, file objects, database connectors, and so on. Since we created a tensor dataset that sits in memory, we are simply assigning `X` and `y` to these attributes, which are placeholders for our tensor objects.\n",
    "\n",
    "In the `__getitem__` method, we define instructions for returning exactly one item from the dataset via an index. This means the features and the class label corresponding to a single training example or test instance.\n",
    "\n",
    "Finally, the `__len__` method constrains instructions for retrieving the length of the dataset. Here, we use the `.shape` attribute of a tensor to return the number of rows in the feature array. In the case of the training dataset, we have five rows, which we can double-check as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe7ed334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ds)), print(len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b29ae",
   "metadata": {},
   "source": [
    "Now that we defined a PyTorch Dataset class we can use for our toy dataset, we can use PyTorch's `DataLoader` class to sample from it, as shown in the code listing below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "92b9040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(235)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db929d6",
   "metadata": {},
   "source": [
    "- **train_loader**: The `ToyDataset` instance created earlier serves as input to the data loader\n",
    "- **shuffle**: Whether or not to shuffle the data\n",
    "- **batch_size**: The number of background processes\n",
    "- **shuffle=False**: It is not necessary to shuffle the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0bcb9",
   "metadata": {},
   "source": [
    "After instantiating the training data loader, we can iterate over it as shown below. (The\n",
    "iteration over the `test_loader` works similarly but is omitted for brevity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fe8138e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-0.9000,  2.9000],\n",
      "        [-1.2000,  3.1000]]) tensor([0, 0])\n",
      "Batch 2: tensor([[-0.5000,  2.6000],\n",
      "        [ 2.3000, -1.1000]]) tensor([0, 1])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2230c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor([[ 2.3000, -1.1000],\n",
      "        [-1.2000,  3.1000]]), tensor([1, 0])]\n",
      "1 [tensor([[ 2.7000, -1.5000],\n",
      "        [-0.5000,  2.6000]]), tensor([1, 0])]\n",
      "2 [tensor([[-0.9000,  2.9000]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "for idx, enu in enumerate(train_loader):\n",
    "    print(idx, enu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f1a02",
   "metadata": {},
   "source": [
    "As we can see based on the output above, the `train_loader` iterates over the training dataset visiting each training example exactly once. This is known as a `training epoch`. Since we seeded the random number generator using `torch.manual_seed(123)` above, you should get the exact same shuffling order of training examples as shown above. However if you iterate over the dataset a second time, you will see that the shuffling order will change. This is desired to prevent deep neural networks getting caught in repetitive update cycles during training.\n",
    "\n",
    "\n",
    "Note that we specified a batch size of `2` above, but the `3rd batch` only contains a single example. That's because we have five training examples, which is not evenly divisible by 2. In practice, having a substantially smaller batch as the last batch in a training epoch can disturb the convergence during training. To prevent this, it's recommended to set `drop_last=True`, which will drop the last batch in each epoch, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1b263d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e49a8",
   "metadata": {},
   "source": [
    "Now, iterating over the training loader, we can see that the last batch is omitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dcd6c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 2: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ae8fd",
   "metadata": {},
   "source": [
    "Lastly, let's discuss the setting `num_workers=0` in the `DataLoader`. This parameter in PyTorch's `DataLoader` function is crucial for `parallelizing` data loading and preprocessing. When `num_workers` is set to `0`, the data loading will be done in the main process and not in separate worker processes. This might seem unproblematic, but it can lead to significant slowdowns during model training when we train larger networks on a GPU. This is because instead of focusing solely on the processing of the deep learning model, the CPU must also take time to load and preprocess the data. As a result, the GPU can sit idle while waiting for the CPU to finish these tasks. In contrast, when `num_workers` is set to a number greater than zero, multiple worker processes are launched to load data in parallel, freeing the main process to focus on training your model and better utilizing your system's resources, which is illustrated in figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763e0b7",
   "metadata": {},
   "source": [
    "![Alt text](../assests/num_workers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346eea6",
   "metadata": {},
   "source": [
    "However, if we are working with very small datasets, setting `num_workers` to `1` or larger may not be necessary since the total training time takes only fractions of a second anyway. On the contrary, if you are working with tiny datasets or interactive environments such as Jupyter notebooks, increasing `num_workers` may not provide any noticeable speedup. They might, in fact, lead to some issues. One potential issue is the overhead of spinning up multiple worker processes, which could take longer than the actual data loading when your dataset is small.\n",
    "\n",
    "\n",
    "Furthermore, for Jupyter notebooks, setting `num_workers` to greater than `0` can sometimes lead to issues related to the sharing of resources between different processes, resulting in errors or notebook crashes. Therefore, it's essential to understand the `trade-off` and make a calculated decision on setting the `num_workers` parameter. When used correctly, it can be a beneficial tool but should be adapted to your specific dataset size and computational environment for optimal results.\n",
    "\n",
    "In real-world experience, setting `num_workers=4` usually leads to optimal performance on many\n",
    "real-world datasets, but optimal settings depend on your hardware and the code used for \n",
    "loading a training example defined in the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d6664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f721e6a0",
   "metadata": {},
   "source": [
    "## A typical training loop\n",
    "\n",
    "Let's now combine all the requirements we have discussed so far and train a neural network on the `toydataset`\n",
    "\n",
    "The training code is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7ae06310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "learning_rate = 0.5\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_index, (features, labels) in enumerate(train_loader):\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # LOGGING\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_index:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train Loss: {loss:.2f}\"         \n",
    "        )\n",
    "\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df72ae",
   "metadata": {},
   "source": [
    "- The dataset from the previous section has `2 features` and `2 classes`\n",
    "- We led the optimizer needs to know which parameters to optimize\n",
    "- Set the gradients from the previous round to zero to prevent unintended gradient accumulation\n",
    "- Compute the gradients to the loss with respect to the model parameters\n",
    "- The optimizer uses the gradients to update the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e730d",
   "metadata": {},
   "source": [
    "As we can see, the loss reaches zero after `3 epochs`, a sign that the model converged on the training set. However, before we evaluate the model's predictions, let's go over some of the details of the preceding code listing.\n",
    "\n",
    "First, note that we initialized a model with `two inputs` and `two outputs`. That's because the `toy dataset` from the previous section has `two input` features and `two class` labels to predict. We used a `stochastic gradient descent (SGD)` optimizer with a `learning rate (lr)` of `0.5`. The learning rate is a `hyperparameter`, meaning it's a tunable setting that we have to experiment with based on observing the loss. Ideally, we want to choose a learning rate such that the loss converges after a certain number of epochs -- the number of epochs is another hyperparameter to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175457d7",
   "metadata": {},
   "source": [
    "In practice, we often use a `third` dataset, a so-called `validation dataset`, to find the optimal\n",
    "`hyperparameter` settings. A `validation dataset` is similar to a `test set`. However, while we only want to use a test set precisely once to avoid biasing the evaluation, we usually use the `validation set` multiple times to tweak the model settings.\n",
    "\n",
    "We also introduced new settings called `model.train()` and `model.eval()`. As these names imply, these settings are used to put the model into a training and an evaluation mode. This is necessary for components that behave differently during training and inference, such as `dropout` or `batch normalization` layers. Since we don't have `dropout` or other components in our `NeuralNetwork` class that are affected by these settings, using `model.train()` and `model.eval()` is redundant in our code above. However, it's best practice to include them anyway to avoid unexpected behaviors when we change the `model architecture` or reuse the code to train a different model.\n",
    "\n",
    "\n",
    "As discussed earlier, we pass the `logits` directly into the `cross_entropy` loss function, which will apply the `softmax function` internally for efficiency and numerical stability reasons. Then, calling `loss.backward()` will calculate the gradients in the computation graph that PyTorch constructed in the background. The `optimizer.step()` method will use the gradients to update the model parameters to minimize the loss. In the case of the `SGD` optimizer, this means multiplying the gradients with the learning rate and adding the scaled negative gradient to the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a3f78",
   "metadata": {},
   "source": [
    "![Alt text](../assests/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a8c74",
   "metadata": {},
   "source": [
    "**PREVENTING UNDESIRED GRADIENT ACCUMULATION** It is important to include an `optimizer.zero_grad()` call in each update round to reset the gradients to zero. Otherwise, the gradients will accumulate, which may be undesired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7d8a0bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n"
     ]
    }
   ],
   "source": [
    "# After we trained the model, we can use it to make predictions, as shown below:\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ed90a",
   "metadata": {},
   "source": [
    "To obtain the class membership probabilities, we can then use PyTorch's softmax function, we follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5b50f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3342c",
   "metadata": {},
   "source": [
    "Let's consider the first row in the code output above. Here, the first value (column) means that the training example has a `99.91%` probability of belonging to `class 0` and a `0.09%` probability of belonging to `class 1`. (The `set_printoptions` call is used here to make the outputs more legible.)\n",
    "\n",
    "\n",
    "We can convert these values into class labels predictions using PyTorch's `argmax` function, which returns the `index position` of the highest value in each row if we set `dim=1` (setting `dim=0` would return the highest value in each column, instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8e977299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(probas, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3ec9b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = torch.argmax(probas, dim=0)\n",
    "# print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5540ce4",
   "metadata": {},
   "source": [
    "Note that it is unnecessary to compute softmax probabilities to obtain the class labels. We could also apply the `argmax` function to the logits (outputs) directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7ab417ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predicitons = torch.argmax(outputs, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60109f2",
   "metadata": {},
   "source": [
    "Above, we computed the predicted labels for the training dataset. Since the training dataset is relatively small, we could compare it to the true training labels by eye and see that the model is 100% correct. We can double-check this using the == comparison operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a8a8dd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions == y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a59854",
   "metadata": {},
   "source": [
    "Using `torch.sum`, we can count the number of correct prediction as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ed3dd599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b584d4",
   "metadata": {},
   "source": [
    "Since the dataset consists of 5 training examples, we have 5 out of 5 predictions that are correct, which equals `5/5 Ã— 100% = 100%` prediction accuracy.\n",
    "\n",
    "However, to generalize the computation of the prediction accuracy, let's implement a `compute_accuracy` function as shown in the following code listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cc7101ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        \n",
    "        predicitons = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predicitons\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "        \n",
    "    return (correct / total_examples).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6ce90",
   "metadata": {},
   "source": [
    "- **compare**: This returns a tensor of True/False values depending on whether the labels match\n",
    "- **correct**: The sum operations counts the number of True values\n",
    "- **return value**: This is the fraction of correct prediction, a value between 0 and 1. And `.item()` returns the value of the tensor as a Python float."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f1335",
   "metadata": {},
   "source": [
    "Note that the following code listing iterates over a data loader to compute the number and fraction of the correct predictions. This is because when we work with large datasets, we typically can only call the model on a small part of the dataset due to memory limitations. The `compute_accuracy` function above is a general method that scales to datasets of arbitrary size since, in each iteration, the dataset chunk that the model receives is the same size as the batch size seen during training.\n",
    "\n",
    "Notice that the internals of the `compute_accuracy` function are similar to what we used before when we converted the logits to the class labels.\n",
    "\n",
    "We can then apply the function to the training as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0a20793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(model, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2355121",
   "metadata": {},
   "source": [
    "Similarly, we can apply the function to the test set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0a15c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c1206",
   "metadata": {},
   "source": [
    "In this section, we learned how we can train a neural network using PyTorch. Next, let's see\n",
    "how we can save and restore models after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf0fd0",
   "metadata": {},
   "source": [
    "## Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf2289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918694d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48deb3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb77a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873db3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2f323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2eaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa462f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b10c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c28447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd1a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36138b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137df13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c13465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb165b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cac603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04431f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076c2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe1e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec12495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a9c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
