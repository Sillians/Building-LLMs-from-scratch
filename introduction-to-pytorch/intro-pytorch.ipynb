{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51993339",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99082b0d",
   "metadata": {},
   "source": [
    "## The three core components of PyTorch\n",
    "\n",
    "Firstly, PyTorch is a tensor library that extends the concept of array-oriented programming\n",
    "library NumPy with the additional feature of accelerated computation on GPUs, thus\n",
    "providing a seamless switch between CPUs and GPUs.\n",
    "\n",
    "Secondly, PyTorch is an automatic differentiation engine, also known as autograd, which\n",
    "enables the automatic computation of gradients for tensor operations, simplifying\n",
    "backpropagation and model optimization.\n",
    "\n",
    "Finally, PyTorch is a deep learning library, meaning that it offers modular, flexible, and\n",
    "efficient building blocks (including pre-trained models, loss functions, and optimizers) for\n",
    "designing and training a wide range of deep learning models, catering to both researchers\n",
    "and developers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfde479",
   "metadata": {},
   "source": [
    "AI is fundamentally about creating computer systems capable of performing tasks that\n",
    "usually require human intelligence. These tasks include understanding natural language,\n",
    "recognizing patterns, and making decisions. (Despite significant progress, AI is still far from\n",
    "achieving this level of general intelligence.)\n",
    "\n",
    "Machine learning represents a subfield of AI that focuses on developing and improving learning algorithms. The key idea behind machine learning is to enable computers to learn from data and make predictions or decisions without being explicitly programmed to perform the task. This involves developing algorithms that can identify patterns and learn from historical data and improve their performance over time with more data and feedback.\n",
    "\n",
    "Machine learning is also behind technologies like recommendation systems used by online retailers and streaming services, email spam filtering, voice recognition in virtual assistants, and even self-driving cars. The introduction and advancement of machine learning have significantly enhanced AI's capabilities, enabling it to move beyond strict rule-based systems and adapt to new inputs or changing environments.\n",
    "\n",
    "Deep learning is a subcategory of machine learning that focuses on the training and\n",
    "application of deep neural networks. These deep neural networks were originally inspired by\n",
    "how the human brain works, particularly the interconnection between many neurons. The\n",
    "\"deep\" in deep learning refers to the multiple hidden layers of artificial neurons or nodes\n",
    "that allow them to model complex, nonlinear relationships in the data.\n",
    "\n",
    "Unlike traditional machine learning techniques that excel at simple pattern recognition,\n",
    "deep learning is particularly good at handling unstructured data like images, audio, or text,\n",
    "so deep learning is particularly well suited for LLMs.\n",
    "\n",
    "Using a learning algorithm, a model is trained on a training dataset consisting of examples\n",
    "and corresponding labels. In the case of an email spam classifier, for example, the training\n",
    "dataset consists of emails and their spam and not-spam labels that a human identified.\n",
    "Then, the trained model can be used on new observations (new emails) to predict their\n",
    "unknown label (spam or not spam)\n",
    "\n",
    "Of course, we also want to add a model evaluation between the training and inference\n",
    "stages to ensure that the model satisfies our performance criteria before using it in a real-\n",
    "world application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2c3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c0b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd03841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce464367",
   "metadata": {},
   "source": [
    "## Understanding tensors\n",
    "\n",
    "Tensors represent a mathematical concept that generalizes vectors and matrices to potentially higher dimensions. In other words, tensors are mathematical objects that can be characterized by their order (or rank), which provides the number of dimensions. For example, a scalar (just a number) is a tensor of rank 0, a vector is a tensor of rank 1, and a matrix is a tensor of rank 2.\n",
    "\n",
    "From a computational perspective, tensors serve as data containers. For instance, they hold multi-dimensional data, where each dimension represents a different feature. Tensor libraries, such as PyTorch, can create, manipulate, and compute with these multi-dimensional arrays efficiently. In this context, a tensor library functions as an array library.\n",
    "\n",
    "\n",
    "PyTorch tensors are similar to NumPy arrays but have several additional features important for deep learning. For example, PyTorch adds an automatic differentiation engine, simplifying computing gradients. PyTorch tensors also support GPU computations to speed up deep neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1a146",
   "metadata": {},
   "source": [
    "## Scalars, vectors, matrices, and tensors\n",
    "\n",
    "As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar\n",
    "is a `0-dimensional` tensor (for instance, just a number), a vector is a `1-dimensional` tensor,\n",
    "and a matrix is a `2-dimensional` tensor. There is no specific term for higher-dimensional\n",
    "tensors, so we typically refer to a `3-dimensional` tensor as just a 3D tensor, and so forth.\n",
    "\n",
    "We can create objects of PyTorch's Tensor class using the torch.tensor function as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2ae749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c63306f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor0d = torch.tensor(1)\n",
    "print(tensor0d, tensor0d.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3626b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) 1\n"
     ]
    }
   ],
   "source": [
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d, tensor1d.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f46fc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4],\n",
      "        [6, 7]]) 2\n"
     ]
    }
   ],
   "source": [
    "tensor2d = torch.tensor([[3, 4], [6, 7]])\n",
    "print(tensor2d, tensor2d.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb59b42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 6],\n",
      "         [3, 7],\n",
      "         [4, 8]]]) 3\n"
     ]
    }
   ],
   "source": [
    "tensor3d = torch.tensor([[[2, 6], [3, 7], [4, 8]]])\n",
    "print(tensor3d, tensor3d.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8a7d9",
   "metadata": {},
   "source": [
    "## Tensor data types\n",
    "\n",
    "PyTorch adopts the default 64-bit integer data type from Python. We can access the data type of a\n",
    "tensor via the `.dtype` attribute of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48461bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfddd8",
   "metadata": {},
   "source": [
    "If we create tensors from Python floats, PyTorch creates tensors with a 32-bit precision by\n",
    "default, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e41693f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = torch.tensor([1.0, 2.3, 4.6])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce77f6",
   "metadata": {},
   "source": [
    "This choice is primarily due to the balance between precision and computational efficiency.\n",
    "A 32-bit floating point number offers sufficient precision for most deep learning tasks, while\n",
    "consuming less memory and computational resources than a 64-bit floating point number.\n",
    "Moreover, GPU architectures are optimized for 32-bit computations, and using this data\n",
    "type can significantly speed up model training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea0b11",
   "metadata": {},
   "source": [
    "Moreover, it is possible to readily change the precision using a tensor's `.to` method. The following code demonstrates this by changing a `64-bit` integer tensor into a `32-bit` float tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c4055a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "vectofloat = tensor1d.to(torch.float32)\n",
    "print(vectofloat, vectofloat.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f0d20",
   "metadata": {},
   "source": [
    "## Common PyTorch tensor operations\n",
    "\n",
    "Some of the most essential PyTorch tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3721d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6],\n",
      "        [ 4,  8],\n",
      "        [ 5, 10]]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# We already introduced the torch.tensor() function to create new tensors.\n",
    "\n",
    "new_tensor = torch.tensor([[3, 6], [4, 8], [5, 10]])\n",
    "print(new_tensor, new_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a39c64c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In addition, the .shape attribute allows us to access the shape of a tensor:\n",
    "\n",
    "new_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e744f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6,  4],\n",
      "        [ 8,  5, 10]])\n"
     ]
    }
   ],
   "source": [
    "# As you can see above, .shape returns [3, 2], which means that the tensor has 3 rows and 2 columns. \n",
    "# To reshape the tensor into a 2 by 3 tensor, we can use the .reshape method:\n",
    "\n",
    "print(new_tensor.reshape(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00791d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6,  4],\n",
      "        [ 8,  5, 10]])\n"
     ]
    }
   ],
   "source": [
    "# However, note that the more common command for reshaping tensors in PyTorch is .view():\n",
    "\n",
    "print(new_tensor.view(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608d574",
   "metadata": {},
   "source": [
    "Similar to `.reshape` and `.view`, there are several cases where PyTorch offers multiple syntax options for executing the same computation.\n",
    "This is because PyTorch initially followed the original `Lua Torch` syntax convention but then also added syntax to make it more similar to NumPy upon popular request.\n",
    "\n",
    "Next, we can use `.T` to transpose a tensor, which means flipping it across its diagonal. Note that this is similar from reshaping a tensor as you can see based on the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6abf5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  4,  5],\n",
      "        [ 6,  8, 10]])\n"
     ]
    }
   ],
   "source": [
    "print(new_tensor.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25d59e",
   "metadata": {},
   "source": [
    "Lastly, the common way to multiply two matrices in PyTorch is the `.matmul` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f14bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33, 40],\n",
      "        [60, 73]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.matmul(tensor2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98248c9",
   "metadata": {},
   "source": [
    "However, we can also adopt the `@` operator, which accomplishes the same thing more compactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25d28a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33, 40],\n",
      "        [60, 73]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d@tensor2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e4a1c",
   "metadata": {},
   "source": [
    "## Seeing models as computation graphs\n",
    "\n",
    "PyTorch's autograd system provides functions to compute gradients in dynamic computational graphs automatically. \n",
    "\n",
    "A computational graph (or computation graph in short) is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network\n",
    "\n",
    "Let's look at a concrete example to illustrate the concept of a computation graph. The following code implements the forward pass (prediction step) of a simple logistic regression classifier, which can be seen as a single-layer neural network, returning a score between `0` and `1` that is compared to the true class label `(0 or 1)` when computing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2aa5a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9183])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2])\n",
    "b = torch.Tensor([0.0])\n",
    "z = x1 * w1 + b\n",
    "\n",
    "y_pred = torch.sigmoid(z)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fec74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loss\n",
    "loss = F.binary_cross_entropy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52bcbdbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0852)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb507a8",
   "metadata": {},
   "source": [
    "![Alt text](../assests/computational-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd924e5",
   "metadata": {},
   "source": [
    "The image above illustrates A logistic regression forward pass as a computation graph. The input feature `x1` is multiplied by a model weight `w1` and passed through an activation function `Ïƒ` after adding the bias. The loss is computed by comparing the model output `a` with a given label `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799a495",
   "metadata": {},
   "source": [
    "In fact, PyTorch builds such a computation graph in the background, and we can use this to\n",
    "calculate gradients of a loss function with respect to the model parameters (here w1 and b)\n",
    "to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658be28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a53b9a5",
   "metadata": {},
   "source": [
    "## Automatic differentiation made easy\n",
    "\n",
    "If we carry out computations in PyTorch, it will build such a graph internally by default if one of its terminal nodes has the `requires_grad` attribute set to True. This is useful if we want to compute `gradients`. Gradients are required when training neural networks via the popular backpropagation algorithm, which can be thought of as an implementation of the `chain rule` from calculus for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deef4bd",
   "metadata": {},
   "source": [
    "![Alt text](../assests/auto-diff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b38d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d44e80b",
   "metadata": {},
   "source": [
    "# PARTIAL DERIVATIVES AND GRADIENTS\n",
    "\n",
    "`Figure A.8` shows partial derivatives, which measure the rate at which a function changes with respect to one of its variables. A gradient is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input.\n",
    "\n",
    "If you are not familiar or don't remember the `partial derivatives`, `gradients`, or the `chain rule` from calculus, don't worry. On a high level, all you need to know for this book is that the chain rule is a way to compute gradients of a loss function with respect to the model's parameters in a computation graph. This provides the information needed to update each parameter in a way that minimizes the loss function, which serves as a proxy for measuring the model's performance, using a method such as gradient descent.\n",
    "\n",
    "the automatic differentiation `(autograd)` engine? By tracking every operation performed on tensors, PyTorch's `autograd` engine constructs a computational graph in the background. Then, calling the `grad` function, we can compute the gradient of the loss with respect to model parameter `w1` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77752504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "z = x1 * w1 + b\n",
    "\n",
    "y_pred = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(y_pred, y)\n",
    "\n",
    "grad_l_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_l_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d870f",
   "metadata": {},
   "source": [
    "By default, PyTorch destroys the computation graph after calculating the gradients to free memory. However, since we are going to reuse this computation graph shortly, we set retain_graph=True so that it stays in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e89caddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0817]),) (tensor([-0.0898]),)\n"
     ]
    }
   ],
   "source": [
    "print(grad_l_b, grad_l_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81e98c",
   "metadata": {},
   "source": [
    "Above, we have been using the grad function `\"manually,\"` which can be useful for experimentation, debugging, and demonstrating concepts. But in practice, PyTorch provides even more high-level tools to automate this process. For instance, we can call `.backward` on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph, which will be stored via the tensors' `.grad` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c99e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5435b0",
   "metadata": {},
   "source": [
    "## Implementing multilayer neural networks\n",
    "\n",
    "In the previous sections, we covered PyTorch's tensor and autograd components. This section focuses on PyTorch as a library for implementing `deep neural networks`. To provide a concrete example, we focus on a `multilayer perceptron`, which is a fully\n",
    "connected neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a1b3a",
   "metadata": {},
   "source": [
    "![Alt text](../assests/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b328b",
   "metadata": {},
   "source": [
    "When implementing a neural network in PyTorch, we typically subclass the `torch.nn.Module` class to define our own custom network architecture. This Module base class provides a lot of functionality, making it easier to build and train models. For instance, it allows us to encapsulate layers and operations and keep track of the model's parameters.\n",
    "\n",
    "Within this subclass, we define the network layers in the `__init__` constructor and specify how they interact in the `forward method`. The forward method describes how the input data passes through the network and comes together as a computation graph.\n",
    "\n",
    "In contrast, the `backward` method, which we typically do not need to implement ourselves, is used during training to compute `gradients` of the `loss function` with respect to the `model parameters`. \n",
    "\n",
    "The following code implements a classic `multilayer perceptron` with two hidden layers to illustrate a typical usage of the Module class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a40f1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(\n",
    "            \n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682997cb",
   "metadata": {},
   "source": [
    "- It's useful to code the number of inputs and outputs as variables to reuse the same code for datasets with different numbers of features and classes.\n",
    "  \n",
    "- The Linear layer takes the number of input and output nodes as arguments.\n",
    "  \n",
    "- Nonlinear activation functions are placed between the hidden layers.\n",
    "  \n",
    "- The number of output nodes of one hidden layer has to match the number of inputs of the next layer.\n",
    "  \n",
    "- The outputs of the last layer are called logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e5055ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a new neural network object as follows:\n",
    "model = NeuralNetwork(50, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa150745",
   "metadata": {},
   "source": [
    "But before using this new model object, it is often useful to call print on the model to see a summary of its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17c17476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e77c0",
   "metadata": {},
   "source": [
    "Note that we used the `Sequential` class when we implemented the `NeuralNetwork` class. Using Sequential is not required, but it can make our life easier if we have a series of layers that we want to execute in a specific order, as is the case here. This way, after instantiating `self.layers = Sequential(...)` in the `__init__` constructor, we just have to call the `self.layers` instead of calling each layer individually in the `NeuralNetwork's` forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b3e7e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "# Next, let's check the total number of trainable parameters of this model:\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable model parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f3b5f",
   "metadata": {},
   "source": [
    "Note that each parameter for which `requires_grad=True` counts as a trainable parameter and will be updated during training.\n",
    "\n",
    "In the case of our `neural network model` with the two hidden layers above, these trainable parameters are contained in the `torch.nn.Linear` layers. A linear layer multiplies the `inputs with a weight matrix and adds a bias vector`. This is sometimes also referred to\n",
    "as a `feedforward` or `fully connected layer`.\n",
    "\n",
    "Based on the `print(model)` call we executed above, we can see that the first Linear layer is at index position `0` in the layers attribute. We can access the corresponding weight parameter matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6ee8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1002,  0.0516,  0.0463,  ..., -0.1280, -0.1195,  0.0278],\n",
      "        [ 0.1319,  0.0549,  0.1327,  ...,  0.0467, -0.0404,  0.1108],\n",
      "        [-0.0768,  0.0059,  0.0921,  ..., -0.0913, -0.1009,  0.0777],\n",
      "        ...,\n",
      "        [-0.1242,  0.0902,  0.0313,  ...,  0.0348, -0.0284,  0.0580],\n",
      "        [-0.0845,  0.1084, -0.0515,  ...,  0.0223,  0.0502,  0.0350],\n",
      "        [ 0.0059, -0.0862, -0.0601,  ...,  0.0730, -0.1253,  0.0554]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfdf19bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0706, -0.1613, -0.0382, -0.0338,  0.0408,  0.0283, -0.1800,  0.0613,\n",
      "          0.0337, -0.1748,  0.0415, -0.1285,  0.0826,  0.1203,  0.1686, -0.0867,\n",
      "         -0.1456, -0.1658, -0.0388,  0.1806, -0.1766,  0.1511, -0.0600,  0.1367,\n",
      "         -0.0169,  0.1419,  0.1603,  0.0325,  0.1771,  0.1390],\n",
      "        [-0.0345, -0.0200, -0.1431,  0.1416, -0.1668, -0.1217,  0.0388,  0.0628,\n",
      "         -0.1759,  0.0356, -0.1225, -0.1818,  0.0592,  0.0677,  0.0149,  0.0779,\n",
      "          0.0678, -0.1519, -0.0082, -0.1400,  0.1643,  0.0040,  0.0731, -0.0616,\n",
      "         -0.0788,  0.1778, -0.0299, -0.0703, -0.1403,  0.0580],\n",
      "        [-0.1791, -0.0134, -0.1733,  0.1272, -0.0072,  0.1291, -0.0544, -0.0058,\n",
      "          0.1186,  0.1017, -0.1676,  0.1193,  0.0153,  0.0970,  0.0720,  0.1543,\n",
      "         -0.0394,  0.0698, -0.0334, -0.0784, -0.0734,  0.0118,  0.0591, -0.0523,\n",
      "          0.0371,  0.0506,  0.1024,  0.1455, -0.0425, -0.0215],\n",
      "        [ 0.1729,  0.0291, -0.0773,  0.0555, -0.0129,  0.0936, -0.0507,  0.1312,\n",
      "         -0.0412,  0.1128,  0.0224,  0.0815,  0.1071, -0.1019,  0.0689,  0.0635,\n",
      "          0.0032, -0.0200,  0.1782,  0.0478, -0.1793,  0.1111,  0.1507, -0.0339,\n",
      "         -0.0842,  0.0776,  0.1028, -0.0634,  0.1683,  0.1774],\n",
      "        [ 0.0897, -0.1527, -0.0023,  0.0621, -0.1655,  0.0062,  0.1813,  0.0050,\n",
      "          0.1479, -0.0681, -0.1116, -0.0995,  0.1503, -0.1600, -0.0795,  0.1131,\n",
      "          0.1508,  0.0512, -0.1590, -0.1477, -0.1628, -0.0971, -0.0545, -0.0701,\n",
      "          0.0511, -0.1470,  0.1398,  0.1482,  0.1093,  0.0295],\n",
      "        [ 0.1034, -0.1324,  0.0522, -0.1763,  0.1302, -0.1013,  0.1153, -0.0675,\n",
      "         -0.0857, -0.1337, -0.1500,  0.0964, -0.1333,  0.0509,  0.0277, -0.0554,\n",
      "         -0.0453,  0.1533, -0.0149, -0.0429,  0.0464, -0.0397,  0.1256,  0.1391,\n",
      "         -0.0057, -0.1631,  0.0756, -0.0831,  0.1691,  0.0215],\n",
      "        [-0.1499,  0.1179, -0.0704,  0.1297,  0.1664,  0.1539, -0.0317,  0.0366,\n",
      "         -0.0618,  0.1341,  0.0178,  0.1610, -0.1077,  0.1518, -0.1720,  0.1321,\n",
      "         -0.0275, -0.0994, -0.0367,  0.0831, -0.0816,  0.0758, -0.0843, -0.1265,\n",
      "          0.0866,  0.0711,  0.0087,  0.1590, -0.0604, -0.0992],\n",
      "        [-0.0320, -0.1323,  0.1372,  0.0776, -0.0064,  0.0733,  0.0792, -0.0549,\n",
      "         -0.1547, -0.0701,  0.1147, -0.0816, -0.1710, -0.1820, -0.0038, -0.0864,\n",
      "          0.1251, -0.0873,  0.0202, -0.0765,  0.0061, -0.0433,  0.1210, -0.1582,\n",
      "          0.0005, -0.0093,  0.1616,  0.1601, -0.0877, -0.0754],\n",
      "        [-0.1749,  0.1736, -0.1189, -0.0411, -0.0259, -0.0722, -0.1471,  0.0117,\n",
      "          0.1427, -0.0150,  0.0765, -0.1705, -0.0619,  0.1796,  0.0292,  0.1224,\n",
      "         -0.0892, -0.1556,  0.0655,  0.0004,  0.1053,  0.0367, -0.1415, -0.0346,\n",
      "          0.1376,  0.1772,  0.0217,  0.1121,  0.1753,  0.0276],\n",
      "        [ 0.1586,  0.0395, -0.0888, -0.0917,  0.0829,  0.1371,  0.0925, -0.1620,\n",
      "          0.0481,  0.0020,  0.0840, -0.0824, -0.1493, -0.0726,  0.1037,  0.0858,\n",
      "          0.0293, -0.0568,  0.1124,  0.1439, -0.1423, -0.0077,  0.1668, -0.0859,\n",
      "          0.1801,  0.1706, -0.1141,  0.1086, -0.0405, -0.1661],\n",
      "        [ 0.1062,  0.0634, -0.0479, -0.0907,  0.0142, -0.1526, -0.0038,  0.0384,\n",
      "         -0.0853, -0.1645, -0.0260,  0.0136,  0.1028, -0.0040,  0.0849,  0.0036,\n",
      "          0.0248,  0.1068, -0.0823, -0.0902,  0.0464, -0.0450, -0.1027,  0.0260,\n",
      "         -0.1082,  0.0817,  0.0647, -0.1065, -0.0768,  0.0737],\n",
      "        [ 0.0849,  0.1004, -0.0867, -0.1265,  0.0142,  0.1115,  0.1409,  0.1192,\n",
      "         -0.0130, -0.0946, -0.0218,  0.0682, -0.0016, -0.0138, -0.1722, -0.0929,\n",
      "         -0.0353,  0.1061, -0.0998,  0.1393,  0.0696,  0.1035, -0.1821,  0.0017,\n",
      "          0.0841,  0.1540, -0.0246,  0.1079, -0.0918, -0.1635],\n",
      "        [ 0.0844,  0.1117,  0.0687,  0.1545, -0.1452,  0.0173, -0.1318,  0.0059,\n",
      "         -0.0548, -0.1170, -0.0484, -0.0916, -0.1011, -0.1305,  0.0735,  0.0122,\n",
      "          0.1239,  0.0705, -0.0486,  0.1518, -0.0565,  0.1265,  0.0677, -0.0586,\n",
      "         -0.1178, -0.0288, -0.0418,  0.0972, -0.0598, -0.0096],\n",
      "        [-0.1074, -0.0727, -0.1825, -0.0593,  0.0615, -0.1191, -0.1365,  0.0442,\n",
      "         -0.1642,  0.1448, -0.1232,  0.0881, -0.1683,  0.0586,  0.1305,  0.0223,\n",
      "         -0.0460,  0.1042,  0.1286, -0.1255,  0.0698, -0.0119,  0.0351, -0.0505,\n",
      "         -0.1026,  0.0266,  0.1299,  0.1392, -0.1489, -0.0094],\n",
      "        [ 0.1153,  0.0918, -0.1033, -0.0933, -0.0782,  0.1199, -0.0665,  0.0503,\n",
      "         -0.0358, -0.1602,  0.0959, -0.0145,  0.0587,  0.0075,  0.0857, -0.0497,\n",
      "          0.0087, -0.0627,  0.1532,  0.0236, -0.0192, -0.0638, -0.0904,  0.1401,\n",
      "         -0.1233,  0.1114,  0.1468,  0.0225,  0.0831, -0.1760],\n",
      "        [-0.1094, -0.0863,  0.1152,  0.1111,  0.0887, -0.0108, -0.1562,  0.0565,\n",
      "         -0.0607, -0.0111,  0.1472, -0.0508, -0.0889, -0.1634,  0.0611,  0.0017,\n",
      "         -0.1137,  0.1816, -0.0555, -0.0662,  0.0979, -0.1635, -0.0165, -0.1511,\n",
      "         -0.0848, -0.0064, -0.0606,  0.0059, -0.1269,  0.0832],\n",
      "        [ 0.1712,  0.0664, -0.0418,  0.0118, -0.0977, -0.0202, -0.1773,  0.1438,\n",
      "         -0.0387,  0.1552,  0.0739,  0.1730,  0.0973,  0.1673,  0.1545,  0.1702,\n",
      "          0.0314,  0.0194, -0.1786,  0.1618,  0.1092, -0.0990, -0.1792,  0.0671,\n",
      "          0.1215,  0.1289,  0.1711, -0.1027,  0.1145,  0.0810],\n",
      "        [ 0.1690, -0.0695, -0.1611, -0.0463, -0.1221, -0.1716,  0.1026, -0.1476,\n",
      "          0.0275, -0.1396,  0.0055, -0.0885,  0.1732,  0.1753, -0.1320,  0.0500,\n",
      "         -0.0054,  0.0075,  0.1339,  0.1185,  0.0270,  0.0884, -0.0401,  0.1405,\n",
      "         -0.0081, -0.1809,  0.1771,  0.1209, -0.0814, -0.0300],\n",
      "        [-0.1314, -0.0350, -0.0043, -0.1036, -0.1620,  0.0065,  0.0105,  0.1210,\n",
      "         -0.1455,  0.1259,  0.0021, -0.0159,  0.1349,  0.1728, -0.0602,  0.1188,\n",
      "         -0.0630, -0.0133,  0.1785, -0.0422,  0.1782, -0.0338,  0.1489,  0.0531,\n",
      "         -0.0590, -0.0728, -0.0856,  0.1204, -0.0073,  0.1049],\n",
      "        [ 0.1494,  0.1246, -0.0551,  0.1672, -0.0232,  0.1538,  0.0206,  0.0391,\n",
      "         -0.0375, -0.1357,  0.1512,  0.0578,  0.0289, -0.0969,  0.1557, -0.1263,\n",
      "         -0.1417, -0.0407, -0.0715,  0.0860,  0.1567,  0.1127,  0.1509,  0.1391,\n",
      "         -0.0537,  0.0404, -0.1807, -0.1542,  0.0430, -0.0171]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e244105",
   "metadata": {},
   "source": [
    "Since this is a large matrix that is not shown in its entirety, let's use the .shape attribute to show its dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78957549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the weight of the first layer: torch.Size([30, 50])\n",
      "Shape of the weight of the second layer: torch.Size([20, 30])\n"
     ]
    }
   ],
   "source": [
    "first_layer = model.layers[0].weight.shape\n",
    "second_layer = model.layers[2].weight.shape\n",
    "\n",
    "print(f\"Shape of the weight of the first layer:\", first_layer)\n",
    "print(f\"Shape of the weight of the second layer:\", second_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3bade",
   "metadata": {},
   "source": [
    "(Similarly, you could access the bias vector via `model.layers[0].bias`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83f6347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the bias of the first layer: torch.Size([30])\n",
      "Shape of the bias of the second layer: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "first_layer_bias = model.layers[0].bias.shape\n",
    "second_layer_bias = model.layers[2].bias.shape\n",
    "\n",
    "print(f\"Shape of the bias of the first layer:\", first_layer_bias)\n",
    "print(f\"Shape of the bias of the second layer:\", second_layer_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f403a",
   "metadata": {},
   "source": [
    "The weight matrix of the first layer above is a `30x50` matrix, and we can see that the `requires_grad` is set to True, which means its entries are trainable -- this is the default setting for `weights` and `biases` in `torch.nn.Linear`.\n",
    "\n",
    "Note that if you execute the code above on your computer, the numbers in the weight matrix will likely differ from those shown above. This is because the model weights are initialized with small random numbers, which are different each time we instantiate the network. In deep learning, initializing model weights with small random numbers is desired to break symmetry during training -- otherwise, the nodes would be just performing the same operations and updates during backpropagation.\n",
    "\n",
    "However, while we want to keep using small random numbers as initial values for our layer weights, we can make the random number initialization reproducible by `seeding` PyTorch's random number generator via `manual_seed`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6b886cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62632995",
   "metadata": {},
   "source": [
    "Now, after we spent some time inspecting the `NeuraNetwork` instance, let's briefly see how it's used via the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ffc44ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "X = torch.rand((1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fcd7b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740, 0.8665, 0.1366, 0.1025, 0.1841,\n",
       "         0.7264, 0.3153, 0.6871, 0.0756, 0.1966, 0.3164, 0.4017, 0.1186, 0.8274,\n",
       "         0.3821, 0.6605, 0.8536, 0.5932, 0.6367, 0.9826, 0.2745, 0.6584, 0.2775,\n",
       "         0.8573, 0.8993, 0.0390, 0.9268, 0.7388, 0.7179, 0.7058, 0.9156, 0.4340,\n",
       "         0.0772, 0.3565, 0.1479, 0.5331, 0.4066, 0.2318, 0.4545, 0.9737, 0.4606,\n",
       "         0.5159, 0.4220, 0.5786, 0.9455, 0.8057]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae725e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50]), 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13de67b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f8795",
   "metadata": {},
   "source": [
    "In the code above, we generated a single random training example `X` as a toy input (note that our network expects `50-dimensional` feature vectors) and fed it to the model, returning three scores. When we call `model(x)`, it will automatically execute the forward pass of the\n",
    "model.\n",
    "\n",
    "The forward pass refers to calculating output tensors from input tensors. This involves passing the input data through all the neural network layers, starting from the input layer, through hidden layers, and finally to the output layer.\n",
    "\n",
    "These three numbers returned above correspond to a score assigned to each of the three output nodes. Notice that the output tensor also includes a `grad_fn` value.\n",
    "\n",
    "Here, `grad_fn=<AddmmBackward0>` represents the last-used function to compute a variable in the computational graph. In particular, `grad_fn=<AddmmBackward0>` means that the tensor we are inspecting was created via a matrix multiplication and addition operation.\n",
    "PyTorch will use this information when it computes gradients during backpropagation. The `<AddmmBackward0>` part of `grad_fn=<AddmmBackward0>` specifies the operation that was performed. In this case, it is an `Addmm` operation. `Addmm` stands for `matrix multiplication (mm)` followed by an `addition (Add)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b43f20",
   "metadata": {},
   "source": [
    "If we just want to use a network without training or backpropagation, for example, if we use it for prediction after training, constructing this computational graph for backpropagation can be wasteful as it performs unnecessary computations and consumes additional memory. So, when we use a model for inference (for instance, making predictions) rather than training, it is a best practice to use the `torch.no_grad()` context manager, as shown below. This tells PyTorch that it doesn't need to keep track of the gradients, which can result in significant savings in memory and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "31bace6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc33ef",
   "metadata": {},
   "source": [
    "In PyTorch, it's common practice to code models such that they return the outputs of the `last layer (logits)` without passing them to a `nonlinear activation` function. That's because `PyTorch's` commonly used loss functions combine the `softmax (or sigmoid for binary\n",
    "classification)` operation with the `negative log-likelihood loss` in a single class. The reason for this is numerical efficiency and stability. So, if we want to compute `class-membership probabilities` for our predictions, we have to call the `softmax` function explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "257156b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)\n",
    "out.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d50b74",
   "metadata": {},
   "source": [
    "The values can now be interpreted as `class-membership probabilities` that sum up to `1`. The values are roughly equal for this random input, which is expected for a randomly initialized model without training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0120d1",
   "metadata": {},
   "source": [
    "## Setting up efficient data loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d55c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b12efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e044bfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9040e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076c2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe1e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec12495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a9c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
