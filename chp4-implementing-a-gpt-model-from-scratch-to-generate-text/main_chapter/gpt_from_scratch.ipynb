{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b4add58",
   "metadata": {},
   "source": [
    "# Implementing a GPT model from Scratch To Generate Text\n",
    "\n",
    "This section covers:\n",
    "\n",
    "- Coding a GPT-like large language model (LLM) that can be trained to generate human-like text.\n",
    "\n",
    "- Normalizing layer activations to stabilize neural network training\n",
    "\n",
    "- Adding shortcut connections in deep neural networks to train models more effectively.\n",
    "\n",
    "- Implementing transformer blocks to create GPT models of various sizes.\n",
    "\n",
    "- Computing the number of parameters and storage requirements of GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca367a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.6\n",
      "torch version: 2.0.1\n",
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed2958",
   "metadata": {},
   "source": [
    "![Alt text](../../assests/figure41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918751e",
   "metadata": {},
   "source": [
    "## Coding an LLM architecture\n",
    "\n",
    "LLMs, such as GPT (Generative Pretrained Transformer), are large deep neural network architectures designed to generate new text one word (or token) at a time. \n",
    "\n",
    "![Alt text](../../assests/figure42.png)\n",
    "\n",
    "- We've covered `Input tokenization and embedding`, `masked multi-head attention` module.\n",
    "\n",
    "- This section focuses on implementing the core structure of the GPT model, including its transformer blocks.\n",
    "- Therefore, these LLMs are often referred to as `\"decoder-like\"` LLMs\n",
    "- Language models are Unsupervised Multitask Learners.\n",
    "- We are scaling up to the size of a small GPT-2 model, specifically the smallest version with `124` million parameters.\n",
    "- In the context of deep learning and LLMs like GPT, the term \"parameters\" refers to the trainable weights of the model. These weights are essentially the internal variables of the model that are adjusted and optimized during the training process to minimize a specific loss function. This optimization allows the model to learn from the training data.\n",
    "- Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7affaa6",
   "metadata": {},
   "source": [
    "- Configuration details for the `124 million` parameter GPT-2 model include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7123d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eee003",
   "metadata": {},
   "source": [
    "We use short variable names to avoid long lines of code later\n",
    "- `\"vocab_size\"` indicates a vocabulary size of `50,257` words, supported by the `BPE` tokenizer discussed in Chapter 2.\n",
    "\n",
    "- `\"context_length\"` represents the model's maximum input token count, as enabled by positional embeddings covered in Chapter 2\n",
    "\n",
    "- `\"emb_dim\"` is the embedding size for token inputs, converting each input token into a `768`-dimensional vector\n",
    "\n",
    "- `\"n_heads\"` is the number of attention heads in the `multi-head attention` mechanism implemented in Chapter 3\n",
    "  \n",
    "- `\"n_layers\"` is the number of transformer blocks within the model, which we'll implement in upcoming sections\n",
    "\n",
    "- `\"drop_rate\"` is the dropout mechanism's intensity, discussed in Chapter 3; `0.1` means dropping `10%` of hidden units during training to mitigate overfitting\n",
    "\n",
    "- `\"qkv_bias\"` decides if the Linear layers in the multi-head attention mechanism (from Chapter 3) should include a bias vector when computing `query (Q)`, `key (K)`, and `value (V)` tensors; we'll disable this option, which is standard practice in modern LLMs; however, we'll revisit this later when loading pretrained GPT-2 weights from OpenAI into our reimplementation in chapter 5\n",
    "\n",
    "\n",
    "\n",
    "Using the configuration above, we will start this section by implementing a `GPT placeholder` architecture (`DummyGPTModel`) in this section, as shown in the figure below. This will provide us with a big-picture view of how everything fits together and what other components we need to code in the upcoming sections to assemble the full GPT model architecture.\n",
    "\n",
    "\n",
    "![Alt text](../../assests/figure43.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb5236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder GPT model architecture class\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
    "        )\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            config[\"emb_dim\"], config[\"vocab_size\"], bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cae92e",
   "metadata": {},
   "source": [
    "The `DummyGPTModel` class above defines a simplified version of a GPT-like model using PyTorch's neural network module (nn.Module). The model architecture in the `DummyGPTModel` class consists of token and positional embeddings, dropout, a series of transformer blocks `(DummyTransformerBlock)`, a final layer normalization `(DummyLayerNorm)`, and a linear output layer `(out_head)`. \n",
    "\n",
    "- The configuration is passed in via a Python dictionary, for instance, the `GPT_CONFIG_124M` dictionary we created earlier.\n",
    "\n",
    "- The `forward` method describes the data flow through the model; it computes `token` and `positional` embeddings for the input indices, applies `dropout`, processes the data through the `transformer` blocks, applies normalization, and finally produces logits with the linear output layer.\n",
    "\n",
    "- The code above is already functional, as we will see later in this section after we prepare the input layer. However, the placeholders `(DummyLayerNorm and DummyTransformerBlock)` for the transformer block and layer normalization, which we will develop in later sections.\n",
    "\n",
    "![Alt text](../../assests/figure44.png)\n",
    "\n",
    "Next, we will prepare the input data and initialize a new GPT model to illustrate its usage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4bccc",
   "metadata": {},
   "source": [
    "- To implement the step in the figure above, we tokenize a batch consisting of two text inputs for the GPT model using the `tiktoken` tokenizer introduced in chap.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "text1 = \"Every effort moves you\"\n",
    "text2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628de503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f13968",
   "metadata": {},
   "source": [
    "- The output above are the resulting token IDs for the two texts. The first row corresponds to the first text, and the second row corresponds to the second text.\n",
    "\n",
    "- Next, we initialize a new `124 million` parameter `DummyGPTModel` instance and feed it the tokenized `batch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9347b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb6770",
   "metadata": {},
   "source": [
    "- The model outputs above, are commonly referred to as logits.\n",
    "\n",
    "- The output tensor has two rows corresponding to the two text samples. Each text sample consists of 4 tokens; each token is a `50,257`-dimensional vector, which matches the size of the tokenizer's vocabulary.\n",
    "\n",
    "- The embedding has `50,257` dimensions because each of these dimensions refers to a unique token in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df36b076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyGPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): DummyTransformerBlock()\n",
       "    (1): DummyTransformerBlock()\n",
       "    (2): DummyTransformerBlock()\n",
       "    (3): DummyTransformerBlock()\n",
       "    (4): DummyTransformerBlock()\n",
       "    (5): DummyTransformerBlock()\n",
       "    (6): DummyTransformerBlock()\n",
       "    (7): DummyTransformerBlock()\n",
       "    (8): DummyTransformerBlock()\n",
       "    (9): DummyTransformerBlock()\n",
       "    (10): DummyTransformerBlock()\n",
       "    (11): DummyTransformerBlock()\n",
       "  )\n",
       "  (final_norm): DummyLayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "905898b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: tok_emb.weight, Shape: torch.Size([50257, 768])\n",
      "Weights: \n",
      "tensor([[ 3.3737e-01, -1.7778e-01, -3.0353e-01,  ..., -3.1813e-01,\n",
      "         -1.3936e+00,  5.2262e-01],\n",
      "        [ 2.5787e-01,  3.4197e-01, -8.1678e-01,  ..., -4.0981e-01,\n",
      "          4.9785e-01, -3.7207e-01],\n",
      "        [ 7.9574e-01,  5.3501e-01,  9.4275e-01,  ..., -1.0749e+00,\n",
      "          9.5492e-02, -1.4138e+00],\n",
      "        ...,\n",
      "        [-7.1278e-01, -5.0190e-01,  1.4119e+00,  ..., -1.4979e-01,\n",
      "         -4.8977e-01, -1.0620e+00],\n",
      "        [ 2.0646e+00,  1.1190e+00,  3.8486e-01,  ..., -7.2015e-01,\n",
      "         -5.5703e-01,  9.8639e-01],\n",
      "        [ 1.1364e-03, -7.5320e-01, -1.7924e-01,  ..., -3.2443e-01,\n",
      "          2.6055e-01,  5.8885e-01]])\n",
      "Layer: pos_emb.weight, Shape: torch.Size([1024, 768])\n",
      "Weights: \n",
      "tensor([[ 0.8769,  0.2550,  0.8441,  ..., -1.0354,  1.3085,  1.7957],\n",
      "        [-1.0029,  0.0995,  1.2459,  ...,  1.5453, -0.1126, -1.5197],\n",
      "        [ 1.3317,  0.7561,  0.9077,  ...,  0.0830,  1.8336, -2.2225],\n",
      "        ...,\n",
      "        [-0.1055, -1.1941, -1.1472,  ..., -1.4544,  0.2918, -0.5483],\n",
      "        [ 0.2218, -0.3332, -1.3375,  ..., -0.4280, -0.0806,  1.7783],\n",
      "        [ 1.1077,  0.0933,  0.8395,  ..., -1.4756,  1.1813,  2.3671]])\n",
      "Layer: out_head.weight, Shape: torch.Size([50257, 768])\n",
      "Weights: \n",
      "tensor([[ 0.0266,  0.0049, -0.0182,  ...,  0.0070, -0.0124, -0.0275],\n",
      "        [ 0.0156, -0.0022, -0.0125,  ..., -0.0274,  0.0311,  0.0285],\n",
      "        [ 0.0044, -0.0063, -0.0033,  ..., -0.0279, -0.0054, -0.0342],\n",
      "        ...,\n",
      "        [-0.0277, -0.0312,  0.0010,  ..., -0.0004,  0.0167, -0.0232],\n",
      "        [-0.0295, -0.0061, -0.0285,  ..., -0.0118, -0.0086, -0.0206],\n",
      "        [-0.0318,  0.0247,  0.0175,  ..., -0.0301, -0.0244,  0.0342]])\n"
     ]
    }
   ],
   "source": [
    "# Access model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "    print(f\"Weights: \\n{param.data}\") # .data to get the tensor values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec40ee",
   "metadata": {},
   "source": [
    "## Normalizing activations with layer normalization\n",
    "\n",
    "- Training deep neural networks with many layers can sometimes prove challenging due to issues like `vanishing` or `exploding` gradients. These issues lead to unstable training dynamics and make it difficult for the network to effectively adjust its weights, which means the learning process struggles to find a set of parameters (weights) for the neural network that minimizes the `loss function`. In other words, the network has difficulty learning the underlying patterns in the data to a degree that would allow it to make accurate predictions or decisions. \n",
    "\n",
    "\n",
    "- We will implement a  `layer normalization` to improve the stability and efficiency of neural network training.\n",
    "- The main idea behind `layer normalization` is to adjust the activations (outputs) of a neural network layer to have a mean of `0` and a variance of `1`, also know as `unit variance`.\n",
    "- This adjustment speeds up the convergence to effective weights and ensures consistent, reliable training. \n",
    "- Layer normalization is applied both before and after the multi-head attention module within the transformer block, which we will implement later.\n",
    "- It's also applied before the final output layer.\n",
    "\n",
    "\n",
    "Here is a visual overview of how layer normalization functions.\n",
    "\n",
    "![Alt text](../../assests/figure45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659d262",
   "metadata": {},
   "source": [
    "- Let's see how layer normalization works by passing a small input sample through a simple neural network layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bec6c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# create 2 training examples with 5 dimensions (features) each\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcde96",
   "metadata": {},
   "source": [
    "- The first row of the output above lists the layer outputs for the first input and the second row lists the layer outputs for the second row:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8f8c8",
   "metadata": {},
   "source": [
    "- The neural network layer above consists of a `Linear` layer followed by a non-linear activation function, `ReLU` (short for Rectified Linear Unit), which is a standard activation function in neural networks. \n",
    "\n",
    "- `ReLU` simply thresholds negative inputs to `0`, ensuring that a layer outputs only positive values, which explains why the resulting layer output does not contain any negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa2b6b",
   "metadata": {},
   "source": [
    "Let's compute the mean and variance for each of the 2 inputs above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=1, keepdim=True)\n",
    "var = out.var(dim=1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0fc2b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape, var.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d040d7d",
   "metadata": {},
   "source": [
    "- The first row in the `mean` tesnor contains the `mean` value for the first input row, and the second output row contains the `mean` for the second input row.\n",
    "\n",
    "- Using `keepdim=True` in operations like mean or variance calculation ensures that the output tensor retains the same number of dimensions as the input tensor, even though the operation reduces the tensor along the dimension specified via `dim`. For instance, without `Keepdim=True`, the returned mean tensor would be a 2-dimensional vector of `[0.1324,\n",
    "0.2170]` instead of a 2Ã—1-dimensional matrix `[[0.1324], [0.2170]]`.\n",
    "\n",
    "- The `dim` parameter specifies the dimension along which the calculation of the statistic (here, mean and variance) should be performed in a tensor.\n",
    "\n",
    "![Alt text](../../assests/figure46.png)\n",
    "\n",
    "\n",
    "From the figure above, for a `2D` tensor (like a matrix), using `dim=-1` for operations such as mean and variance calculation is the same as using `dim=1`. This is beacuase `-1` refers to the tensor's last dimension, which corresponds to the columns in a `2D` tensor. \n",
    "\n",
    "- Later, when adding layer normalization to the GPT model, which produces `3D` tensors with shape `[batch_size, num_tokens, embedding_size]`, we can still use `dim=-1` for normalization across the last dimension, avoiding a change from `dim=1` to `dim=2`.\n",
    "\n",
    "- Let us apply layer normalization to the layer outputs we obtained earlier. The operation consists of substracting the mean and dividing by the `square-root` of the variance (also known as `standard deviation`). This centers the inputs to have a mean of `0` and a variance of `1` across the column (feature) dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb59dfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[2.9802e-08],\n",
      "        [3.9736e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a797b0",
   "metadata": {},
   "source": [
    "- The normalized layer outputs above contain negative values, have zero mean and a variance of 1.\n",
    "\n",
    "- To improve readability, we can also turn off the scientific notation when printing tensor values by setting `sci_mode` to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d75eb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307a887",
   "metadata": {},
   "source": [
    "- Above we normalized the features of each input.\n",
    "- Now, using the same idea, we can implement a `LayerNorm` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d00a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d602f09",
   "metadata": {},
   "source": [
    "- This specific implementation of `layer Normalization` operates on the last dimension of the input tensor `x`, which represents the embedding dimension `(emb_dim)`. The variable `eps` is a small constant `(epsilon)` added to the variance to prevent division by zero during normalization.\n",
    "\n",
    "**Scale and shift**\n",
    "\n",
    "- Note that in addition to performing the normalization by subtracting the mean and dividing by the variance, we added two trainable parameters, a `scale` and a `shift` parameter.\n",
    "\n",
    "- The initial `scale` (multiplying by 1) and `shift` (adding 0) values don't have any effect; however, `scale` and `shift` are trainable parameters that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task.\n",
    "\n",
    "- This allows the model to learn appropriate scaling and shifting that best suit the data it is processing.\n",
    "\n",
    "- Note that we also add a smaller value `(eps)` before computing the square-root of the variance; this is to avoid division-by-zero errors if the variance is `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f9eae",
   "metadata": {},
   "source": [
    "**BIASED VARIANCE**\n",
    "\n",
    "- In the variance calculation above, setting `unbiased=False` means using the formula:\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n",
    "\n",
    "to compute the variance, where $n$ is the sample size (here, the number of features or columns). This formula does not include Bessel's correction (which uses $n-1$ in the denominator), thus providing a **biased estimate** of the population variance.\n",
    "\n",
    "- For LLMs, where the embedding dimension `n` is very large, the difference between using `n` and `n-1` is negligible.\n",
    "\n",
    "- However, `GPT-2` was trained with a biased variance in the normalization layers, which is why we also adopted this setting for compatibility reasons with the pretrained weights that we will load in later section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad054a8",
   "metadata": {},
   "source": [
    "- Let's now try the LayerNorm module in practice and apply it to the batch input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7185745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a4066",
   "metadata": {},
   "source": [
    "![Alt text](../../assests/figure47.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e664cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082b801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e62e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cce6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dac144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18be38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852126dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151710b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21703cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56ede7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708fdbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8c3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7c7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2e924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d6895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c411866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad64a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d05b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7861ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916fdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cd453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5828415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a8696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae9cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa064a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad569a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9e8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
