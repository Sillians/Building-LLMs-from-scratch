{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557c464a",
   "metadata": {},
   "source": [
    "# **Gaussian Error Linear Unit (GELU) Activation**\n",
    "\n",
    "- `\"GELU\"` most commonly refers to the `Gaussian Error Linear Unit`, a smooth, non-monotonic activation function used in deep learning models like `Transformers` to improve performance by weighting inputs based on their probability under a standard normal distribution, rather than gating them by sign as `ReLU` does. The `GELU` function is mathematically represented as $x \\cdot \\Phi(x)$, where $\\Phi(x)$ is the standard Gaussian cumulative distribution function (CDF). \n",
    "\n",
    "\n",
    "- The `Gaussian Error Linear Unit (GELU)` is a smooth, non-linear activation function introduced by Hendrycks & Gimpel (2016). Unlike `ReLU` (which either keeps or drops values), `GELU` weights inputs by their `probability of being significant`, based on the `Gaussian cumulative distribution function (CDF)`.\n",
    "\n",
    "- It’s widely used in `Transformers (BERT, GPT, ViT, etc.)` because it combines the benefits of `ReLU’s sparsity` and `sigmoid’s smoothness`.\n",
    "\n",
    "\n",
    "### **Intuition**\n",
    "\n",
    "- Instead of `hard-thresholding` like ReLU ($x \\mapsto \\max(0, x)$), GELU makes a `soft decision`:\n",
    "\n",
    "    - Small/negative values are mostly suppressed.\n",
    "    - Large positive values pass through nearly unchanged.\n",
    "    - Near-zero values are partially preserved depending on probability.\n",
    "\n",
    "- This makes GELU `smooth`, `differentiable everywhere`, and well-suited for gradient-based optimization in deep models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99690123",
   "metadata": {},
   "source": [
    "### **Mathematical Representation**\n",
    "\n",
    "Exact Definition\n",
    "\n",
    "$\\mathrm{GELU}(x) = x \\cdot \\Phi(x)$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\Phi(x) = \\frac{1}{2}\\left[1 + \\mathrm{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$\n",
    "\n",
    "- $\\mathrm{erf}(\\cdot)$ = Gaussian error function.\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "- $\\mathrm{GELU}(x) = \\frac{1}{2}x \\left[1 + \\mathrm{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$\n",
    "\n",
    "\n",
    "**Approximation (fast version used in practice)**\n",
    "\n",
    "$\\mathrm{GELU}(x) \\approx 0.5x \\left[1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right)\\right]$\n",
    "\n",
    "This avoids computing the slow error function $\\mathrm{erf}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81eef69",
   "metadata": {},
   "source": [
    "### **Key Properties**\n",
    "\n",
    "- `Smooth and differentiable everywhere` (unlike ReLU).\n",
    "\n",
    "- `Probabilistic interpretation:` input is weighted by probability of being “active.”\n",
    "\n",
    "- `Combines linear & non-linear behavior:` behaves like identity for large $x$, suppresses negatives like ReLU.\n",
    "\n",
    "- `Empirical success:` BERT, GPT, and Vision Transformers all use GELU by default.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Use Cases**\n",
    "\n",
    "- `Transformers:` BERT, GPT-family, ViT.\n",
    "\n",
    "- `NLP tasks:` embeddings, encoder/decoder feedforward networks.\n",
    "\n",
    "- `Vision:` ResMLPs, MLP-Mixers, ViT.\n",
    "\n",
    "- `General deep learning:` MLPs where smooth activation helps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb820e",
   "metadata": {},
   "source": [
    "**GELU from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deafdfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0036, -0.0225, -0.0798, -0.1588, -0.1232,  0.2102,  0.8412,  1.5869,\n",
       "         2.3108,  2.9964])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"GELU activation function\"\"\"\n",
    "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "                                    (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "x = torch.linspace(-3, 3, 10)\n",
    "gelu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698fa21",
   "metadata": {},
   "source": [
    "**GELU in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78a5807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU: tensor([-0.0040, -0.0229, -0.0797, -0.1587, -0.1231,  0.2102,  0.8413,  1.5870,\n",
      "         2.3104,  2.9960])\n",
      "Approx GELU: tensor([-0.0036, -0.0225, -0.0798, -0.1588, -0.1232,  0.2102,  0.8412,  1.5869,\n",
      "         2.3108,  2.9964])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.linspace(-3, 3, 10)\n",
    "\n",
    "# Using PyTorch built-in GELU\n",
    "gelu = nn.GELU()\n",
    "print(\"GELU:\", gelu(x))\n",
    "\n",
    "# Using functional API (approximate version)\n",
    "print(\"Approx GELU:\", F.gelu(x, approximate=\"tanh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86b6c9",
   "metadata": {},
   "source": [
    "**GELU in a Transformer Feedforward Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ceec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.act(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a380f3",
   "metadata": {},
   "source": [
    "**Hugging Face Transformer Config (BERT example)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61dc8c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation function: gelu\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"Activation function:\", model.config.hidden_act)  # 'gelu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf8518",
   "metadata": {},
   "source": [
    "### **Visual Intuition**\n",
    "\n",
    "Compared to ReLU and Sigmoid:\n",
    "\n",
    "- `ReLU:` sharp cutoff at 0, discards all negatives.\n",
    "\n",
    "- `Sigmoid:` squashes everything into (0,1), not scale-preserving.\n",
    "\n",
    "- `GELU:` keeps large positives, smoothly suppresses negatives, probabilistic near zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36258341",
   "metadata": {},
   "source": [
    "### **Summary**\n",
    "\n",
    "`GELU` is the default activation function in Transformers because it is `smooth`, `probabilistic`, and empirically better than ReLU in `NLP/vision`. Its definition uses the Gaussian CDF, with a fast $\\tanh$ approximation widely used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e27f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
