{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abc1d19",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### **What is Tokenization?**\n",
    "\n",
    "`Tokenization` is the process of breaking down a raw text string into smaller, meaningful units called `tokens`. These tokens are the basic `\"words\"` or `\"sub-words\"` that a model can understand and process. It's the bridge between human language and machine numerical representation.\n",
    "\n",
    "Think of it like this: you can't do math with the word `\"seven,\"` you need the number `7`. Similarly, an AI model can't process the word `\"hello,\"` it needs a numerical ID, say `42`. Tokenization is the process that maps `\"hello\"` to `42`.\n",
    "\n",
    "\n",
    "\n",
    "**Why is it Necessary?**\n",
    "\n",
    "- `Numerical Representation:` Machine Learning models are mathematical functions; they require numbers as input, not strings. Tokenization converts text into integer indices.\n",
    "\n",
    "- `Handling Vocabulary:` It defines a model's known set of words (its vocabulary). Any word not in this vocabulary is a problem, which modern tokenizers solve cleverly.\n",
    "\n",
    "- `Efficiency:` Processing individual tokens is more efficient than processing whole documents or characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ed59d",
   "metadata": {},
   "source": [
    "### **Levels of Tokenization**\n",
    "\n",
    "There are three main approaches, each with pros and cons.\n",
    "\n",
    "**1. Word-based Tokenization**\n",
    "\n",
    "This splits text into words based on spaces and punctuation.\n",
    "\n",
    "- **Example:** \"Don't hesitate to ask!\" becomes `[\"Don\", \"t\", \"hesitate\", \"to\", \"ask\"]`\n",
    "\n",
    "- **Pros:** Intuitive, tokens have clear meaning.\n",
    "\n",
    "- **Cons:**\n",
    "  - **Large Vocabulary:** A model must know every possible word, inflection, and misspelling, leading to huge vocabularies (100k+ tokens).\n",
    "  - **Out-of-Vocabulary (OOV) Problem:** What happens with the word `\"Supercalifragilisticexpialidocious\"?` It's unseen, so it becomes an `[UNK] (unknown)` token, losing all meaning.\n",
    "  - **No semantic similarity:** Words like \"run\" and \"running\" are treated as entirely different tokens.\n",
    "\n",
    "\n",
    "**2. Character-based Tokenization**\n",
    "\n",
    "This splits text into individual characters.\n",
    "\n",
    "- **Example:** `\"ask\"` becomes `[\"a\", \"s\", \"k\"]`\n",
    "\n",
    "- **Pros:**\n",
    "  - **Tiny Vocabulary:** Only `~26 letters` (plus punctuation), so very small vocabulary size.\n",
    "  - **No OOV Problem:** Any word can be built from characters.\n",
    "\n",
    "- **Cons:**\n",
    "  - **Loss of Meaning:** Individual characters carry very little semantic meaning.\n",
    "  - **Long Sequences:** \"hello\" is 1 word but 5 tokens, making processing much longer and more expensive for the model.\n",
    "\n",
    "\n",
    "**3. Subword-based Tokenization (The Modern Standard)**\n",
    "\n",
    "This is the best of both worlds. It splits words into frequent sub-word units (like prefixes, suffixes, and roots). Rare words are split into meaningful sub-tokens, while common words remain a single token.\n",
    "\n",
    "\n",
    "- **Example:** The word `\"unhappily\"` might be split into `[\"un\", \"happ\", \"ily\"]`—all meaningful units that the model has likely seen before in other words like `\"untrue\"`, `\"happy\"`, and `\"quickly\"`.\n",
    "\n",
    "- **Pros:**\n",
    "  - **Balanced Vocabulary:** Vocabulary size is a chosen compromise (e.g., 30k-100k tokens).\n",
    "  - **Solves OOV Problem:** Effectively zero unknown tokens. Even a new word like `\"tokenization\"` can be split into known subwords: `[\"token\", \"ization\"]`.\n",
    "  - **Low unknown tokens:** Covers most language constructs effectively.\n",
    "\n",
    "- **Cons:** Less intuitive for humans to read than whole words.\n",
    "\n",
    "`Algorithms that use this:` \n",
    "- `Byte-Pair Encoding` (BPE - used by GPT), \n",
    "- `WordPiece` (used by BERT), \n",
    "- `SentencePiece` (used by Llama). Used in multilingual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22f38a",
   "metadata": {},
   "source": [
    "### The Tokenization Pipeline: A Step-by-Step Example\n",
    "\n",
    "Let's tokenize the sentence: `\"I don't like sand.\"` using a subword tokenizer (like `GPT-2's`).\n",
    "\n",
    "**Step 1: Pre-tokenization**\n",
    "\n",
    "Split the text into rough, word-like chunks based on rules and punctuation.\n",
    "`[\"I\", \"don't\", \"like\", \"sand\", \".\"]`\n",
    "\n",
    "\n",
    "**Step 2: Apply the Tokenization Algorithm (e.g., BPE)**\n",
    "\n",
    "The tokenizer checks each `\"word\"` against its pre-learned merge rules.\n",
    "\n",
    "- `\"I\"` -> Found in vocabulary. Becomes token **314**.\n",
    "\n",
    "- `\"don't\"` -> Not found. It's split into subwords: `[\"do\", \"n't\"]`. These are found. Become tokens **1120**, **426**.\n",
    "\n",
    "- `\"like\"` -> Found. Becomes token **544**.\n",
    "\n",
    "- `\"sand\"` -> Found. Becomes token **5933**.\n",
    "\n",
    "- `\".\"` -> Found. Becomes token **13**.\n",
    "\n",
    "\n",
    "**Step 3: Add Special Tokens (Model-Specific)**\n",
    "\n",
    "Many models add special tokens to signify the start, end, or other information.\n",
    "`[**50256**, 314, 1120, 426, 544, 5933, 13, **50256**]` (`50256` might be an `<|endoftext|>` token)\n",
    "\n",
    "**Final Result:** The original string is now a list of integers: `[50256, 314, 1120, 426, 544, 5933, 13, 50256]`\n",
    "This list is what is fed directly into the model's embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481abaf",
   "metadata": {},
   "source": [
    "### Key Concepts to Remember\n",
    "\n",
    "\n",
    "- **Vocabulary:** The fixed lookup table that maps token strings (e.g., \" like\") to their integer ID (e.g., `544`). It's created during the training of the tokenizer on a large corpus.\n",
    "\n",
    "- **Special Tokens:** Tokens that don't correspond to text but have a functional purpose:\n",
    "\n",
    "  - `[CLS] (BERT):` Classification token. Its final hidden state is used for classification tasks.\n",
    "\n",
    "  - `[SEP] (BERT):` Separator token, used to separate two sentences.\n",
    "\n",
    "  - `[UNK]:` Represents an `unknown` word that couldn't be tokenized. Good tokenizers minimize this.\n",
    "\n",
    "  - `[PAD]:` Padding token, used to make all sequences in a batch the same length.\n",
    "\n",
    "- **Attention to Spaces:** Notice how in the example, the token for `\"like\"` might be `\" like\"` (with a space) in the vocabulary. This is common in `BPE-based `tokenizers` to distinguish where words start. This is why you should use the exact tokenizer a model was trained with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db8872",
   "metadata": {},
   "source": [
    "- Python Code Example with `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a119c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb67eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'Ġdon', \"'t\", 'Ġlike', 'Ġsand', '.']\n",
      "Input IDs: [40, 836, 470, 588, 6450, 13]\n",
      "Decoded: I don't like sand.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for a specific model (e.g., GPT-2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Our text\n",
    "text = \"I don't like sand.\"\n",
    "\n",
    "# Tokenization pipeline: text -> tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens) \n",
    "# Output: ['I', 'Ġdon', \"'\", 't', 'Ġlike', 'Ġsand', '.']\n",
    "# The 'Ġ' is a special character representing a space.\n",
    "\n",
    "# Convert tokens to IDs (what the model actually sees)\n",
    "input_ids = tokenizer.encode(text)\n",
    "print(\"Input IDs:\", input_ids) \n",
    "# Output: [40, 1107, 11, 284, 1447, 7966, 13] \n",
    "# (These numbers are specific to GPT-2's vocabulary)\n",
    "\n",
    "# Decode back to text to verify\n",
    "decoded_text = tokenizer.decode(input_ids)\n",
    "print(\"Decoded:\", decoded_text) \n",
    "# Output: \"I don't like sand.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d39da0",
   "metadata": {},
   "source": [
    "In summary, `tokenization` is a non-trivial but crucial process that transforms messy human text into a clean, numerical structure that machine learning models can consume, and `subword tokenization` is the powerful technique that makes modern LLMs so effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a2938",
   "metadata": {},
   "source": [
    "### More Examples\n",
    "\n",
    "**Step 1: Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee744a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Learning', 'token', '##ization', 'is', 'essential', 'in', 'NL', '##P', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"Learning tokenization is essential in NLP.\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e7487",
   "metadata": {},
   "source": [
    "**Step 2: Conversion to Input IDs**\n",
    "\n",
    "Each token is mapped to a unique ID from the tokenizer’s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3405ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9681, 22559, 2734, 1110, 6818, 1107, 21239, 2101, 119]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9f454",
   "metadata": {},
   "source": [
    "**Step 3: Decoding**\n",
    "\n",
    "The decode method reverses the process, converting IDs back into readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning tokenization is essential in NLP.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1cd3a",
   "metadata": {},
   "source": [
    "#### Using Pretrained Tokenizers\n",
    "\n",
    "Hugging Face’s `transformers` library simplifies working with tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bacb981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_tokenizer_directory/tokenizer_config.json',\n",
       " 'my_tokenizer_directory/special_tokens_map.json',\n",
       " 'my_tokenizer_directory/vocab.txt',\n",
       " 'my_tokenizer_directory/added_tokens.json',\n",
       " 'my_tokenizer_directory/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading a Tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Saving a Tokenizer\n",
    "tokenizer.save_pretrained(\"my_tokenizer_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e338b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1706, 6378, 2734, 1110, 2501, 1106, 21239, 2101, 8249, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Combining Tokenization and Conversion\n",
    "\n",
    "encoded = tokenizer(\"Tokenization is key to NLP tasks.\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24c601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5140a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
