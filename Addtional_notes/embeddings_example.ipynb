{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e5f807",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "`nn.Embedding` in PyTorch is a module that serves as a trainable lookup table, primarily used to convert discrete input (like integer indices representing words or categories) into dense, continuous vector representations, known as `embeddings`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325af45",
   "metadata": {},
   "source": [
    "In PyTorch, an `Embedding layer` is used to convert input indices into dense vectors of fixed size. It's commonly used in natural language processing (NLP) tasks, where words or tokens are represented as integers (indices), and the goal is to map these indices into continuous vector spaces.\n",
    "\n",
    "**Lookup Table:**\n",
    "\n",
    "It functions as a matrix where each row corresponds to an embedding vector for a specific index. When an input index is provided, nn.Embedding retrieves the corresponding row `(embedding vector)` from this matrix.\n",
    "\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "The `Embedding` layer is designed to transform discrete tokens (like words) into continuous vectors. Instead of representing words as one-hot encoded vectors, which can be `sparse` and `high-dimensional`, an embedding layer represents each word as a `low-dimensional`, `dense vector`. This helps the model learn relationships between different words, as similar words can have similar vector representations.\n",
    "\n",
    "\n",
    "**Key Features**\n",
    "\n",
    "- `Input:` The input to the `Embedding` layer is usually a tensor of indices, where each index corresponds to a specific word or token.\n",
    "\n",
    "- `Embedding Matrix:` Inside the embedding layer, PyTorch maintains a matrix where each row corresponds to the vector representation of a token. This matrix is initialized randomly (or using pretrained embeddings) and is updated during training.\n",
    "\n",
    "- `Output:` The output of the embedding layer is a tensor where each index in the input has been replaced by its corresponding vector from the embedding matrix.\n",
    "\n",
    "\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- `num_embeddings:` The size of the dictionary of embeddings, i.e., the total number of unique items (e.g., words in a vocabulary) that can be embedded.\n",
    "\n",
    "- `embedding_dim:` The size of each embedding vector, i.e., the dimensionality of the continuous representation for each item.\n",
    "\n",
    "\n",
    "\n",
    "**Trainable:**\n",
    "\n",
    "The embedding vectors within the nn.Embedding layer are typically initialized randomly and are updated during the training process through backpropagation, allowing the model to learn meaningful representations based on the task at hand (e.g., natural language processing tasks like sentiment analysis or machine translation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d492275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5880,  0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792,  0.7671],\n",
      "        [-1.1925,  0.6984, -1.4097],\n",
      "        [ 0.1794,  1.8951,  1.3689]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define an Embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
    "\n",
    "# Example input (batch of token indices)\n",
    "input_indices = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "# Forward pass through the embedding layer\n",
    "output = embedding_layer(input_indices)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b83b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c49e87",
   "metadata": {},
   "source": [
    "**In the examples above:**\n",
    "\n",
    "- `num_embeddings` is the size of the vocabulary (how many distinct tokens/words you have).\n",
    "\n",
    "- `embedding_dim` is the size of the vector space each token is embedded into.\n",
    "\n",
    "- `input_indices` are indices of tokens, and the output will be a tensor containing their corresponding embeddings.\n",
    "\n",
    "\n",
    "\n",
    "**Key Benefits**\n",
    "\n",
    "- `Dense representations:` The embedding vectors are dense and typically much lower in dimensionality than one-hot encodings.\n",
    "\n",
    "- `Learning semantic relationships:` During training, the model learns meaningful relationships between tokens. For example, in NLP, similar words tend to have similar embeddings.\n",
    "\n",
    "\n",
    "\n",
    "**Applications**\n",
    "\n",
    "It is widely used in `Natural Language Processing (NLP)` to represent words, subwords, or other categorical features as `dense vectors`, enabling neural networks to process and understand text data more effectively than with `sparse representations` like `one-hot encodings`. It can also be used for other types of categorical data in various machine learning applications.\n",
    "\n",
    "\n",
    "- `Word Embeddings in NLP:` Words can be embedded into a continuous space, allowing the model to better understand relationships between words (e.g., using embeddings like `Word2Vec` or `GloVe`).\n",
    "\n",
    "- `Categorical Data:` Embeddings can also be applied to other forms of discrete categorical data in different domains.\n",
    "\n",
    "\n",
    "**Understanding the Output**\n",
    "\n",
    "**1. Shape of the Output:** The output tensor is of shape `(4, 3)`. This means there are `4 rows`, each representing the embedding of one input index, and each embedding is a `3-dimensional vector`.\n",
    "\n",
    "- The number of rows corresponds to the number of input indices (in this case, `[1, 2, 3, 4]`, i.e., 4 indices).\n",
    "\n",
    "- The number of columns (3) corresponds to the `embedding_dim`, which defines the size of each embedding vector.\n",
    "\n",
    "\n",
    "\n",
    "**2. Values in the Output:** The values in the output tensor are the `embedding vectors` assigned to each input index. Each vector is learned during training, initialized randomly in this case (since we havenâ€™t trained the model yet).\n",
    "\n",
    "  - First Row: `[-0.5880,  0.3486,  0.6603]` is the embedding vector for the token with index `1`.\n",
    "  \n",
    "  - Second Row: `[-0.2196, -0.3792,  0.7671]` is the embedding vector for the token with index `2`.\n",
    "\n",
    "  - Third Row: `[-1.1925,  0.6984, -1.4097]` is the embedding vector for the token with index `3`.\n",
    "\n",
    "  - Fourth Row: `[ 0.1794,  1.8951,  1.3689]` is the embedding vector for the token with index `4`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6c73d",
   "metadata": {},
   "source": [
    "These vectors are initially random, and during the training process, the model will update these vectors to capture meaningful information about the relationships between the tokens.\n",
    "\n",
    "`grad_fn=<EmbeddingBackward0>:` This part indicates that the embedding operation is differentiable, and this tensor's values are computed in a way that allows gradients to be tracked. The `grad_fn=<EmbeddingBackward0>` shows that this operation supports `backpropagation`, meaning the values in the embedding matrix will be updated during training.\n",
    "\n",
    "\n",
    "**To summarize:**\n",
    "\n",
    "- The output contains the embeddings (dense vectors) for the input indices `[1, 2, 3, 4]`.\n",
    "\n",
    "- Each index is mapped to a unique 3-dimensional vector (since `embedding_dim=3`).\n",
    "\n",
    "- The embeddings are currently random but will be learned during training as the model optimizes its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9ed1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b4cfb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3178, -1.4041, -1.6237, -3.1146, -1.2449],\n",
      "         [ 0.8456, -1.3980,  1.0862, -0.8557,  0.7466],\n",
      "         [-0.1966,  0.2221,  1.7297, -0.1098,  1.2997],\n",
      "         [ 0.4546,  1.4348, -1.8808,  1.0109, -0.3142]],\n",
      "\n",
      "        [[ 3.3002,  1.0113, -0.3251, -0.8544,  0.7233],\n",
      "         [ 0.4546,  1.4348, -1.8808,  1.0109, -0.3142],\n",
      "         [ 0.8456, -1.3980,  1.0862, -0.8557,  0.7466],\n",
      "         [ 1.3938,  3.2831,  0.7804, -1.8471, -0.3983]],\n",
      "\n",
      "        [[ 1.5156,  1.9463,  0.7986, -0.8951,  0.0356],\n",
      "         [ 0.8456, -1.3980,  1.0862, -0.8557,  0.7466],\n",
      "         [-0.1966,  0.2221,  1.7297, -0.1098,  1.2997],\n",
      "         [ 0.4820, -0.7725,  0.1360,  0.3886, -0.5229]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define an embedding layer for a vocabulary of 10 words, with each embedding being 5-dimensional\n",
    "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=5)\n",
    "\n",
    "# Input a tensor of word indices\n",
    "input_indices = torch.tensor([[0, 2, 5, 4],\n",
    "                              [6, 4, 2, 3],\n",
    "                              [8, 2, 5, 9]])\n",
    "\n",
    "# Get the embeddings for the input indices\n",
    "embeddings = embedding_layer(input_indices)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37355214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226f5a6",
   "metadata": {},
   "source": [
    "The output shape `torch.Size([3, 4, 5])` from an embedding layer means:\n",
    "\n",
    "- **3**: Batch size (number of sequences)\n",
    "- **4**: Sequence length (number of tokens per sequence)  \n",
    "- **5**: Embedding dimension (size of each token's vector representation)\n",
    "\n",
    "Each token in the input is replaced with a 5-dimensional dense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
