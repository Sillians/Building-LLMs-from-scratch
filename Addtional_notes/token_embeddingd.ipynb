{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202f6105",
   "metadata": {},
   "source": [
    "# **Token Embedding in Transformers**\n",
    "\n",
    "\n",
    "Token embeddings map discrete text tokens (subwords, characters, bytes) into continuous vectors so attention and neural layers can operate in $\\mathbb{R}^d$. They form the modelâ€™s primary lexical interface: input embeddings are consumed by the Transformer body, and (often) output logits are produced by projecting Transformer outputs back to token logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef127670",
   "metadata": {},
   "source": [
    "## **Types & tokenization (concise)**\n",
    "\n",
    "- `One-hot + embedding matrix (canonical)`\n",
    "  \n",
    "- `Subword tokenization`\n",
    "  \n",
    "- `Byte-level / character-level`\n",
    "  \n",
    "- `Hybrid / morpheme-aware`\n",
    "  \n",
    "- `Contextual embeddings (layered)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9515b",
   "metadata": {},
   "source": [
    "## **Mathematical representation (essential formulas)**\n",
    "\n",
    "- Embedding lookup (index $t$):\n",
    "$$e_t = E_{i_t} \\quad\\text{or}\\quad e_t = E[i_t]$$\n",
    "where $i_t$ is token id and $E \\in \\mathbb{R}^{V\\times D}$.\n",
    "\n",
    "\n",
    "- Input to Transformer (with positional term):\n",
    "$$x_t = e_t + PE_t$$\n",
    "or, more generally, $X = E[\\text{tokens}] + PE$.\n",
    "\n",
    "\n",
    "- Output logits (untied):\n",
    "$$z = W^\\top h + b,\\quad W \\in \\mathbb{R}^{D\\times V}$$\n",
    "\n",
    "\n",
    "- Weight tying / shared input-output (common):  \n",
    "$$W = E^\\top$$ so $$z = E h$$ (reduces params, often improves calibration).\n",
    "\n",
    "\n",
    "- Softmax probability:\n",
    "$$\n",
    "p = \\mathrm{softmax}(z)\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333299d7",
   "metadata": {},
   "source": [
    "### **One-hot + embedding matrix (canonical)**\n",
    "\n",
    "- tokens indexed by integers; lookup into a matrix $E \\in \\mathbb{R}^{V\\times D}$ produces vectors. Often used with subword vocabularies.\n",
    "\n",
    "- The most direct way to show token embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098f6bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Token IDs (batch of 3 tokens)\n",
    "token_ids = torch.tensor([1, 42, 999])\n",
    "embeds = embedding(token_ids)\n",
    "print(embeds.shape)  # (3, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2ffc4",
   "metadata": {},
   "source": [
    "### **Subword tokenization** \n",
    "\n",
    "- `BPE`, `WordPiece`, `Unigram`: compromise between vocabulary size and ability to represent `OOVs`; most modern Transformers use subwords.\n",
    "- Most Transformers today use subword tokenization (e.g., BPE in GPT, WordPiece in BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfafdd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 19081,  2024,  3928,  1012,   102]])\n",
      "torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Example: BERT tokenizer (WordPiece)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Transformers are powerful.\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Token IDs\n",
    "print(tokens[\"input_ids\"])\n",
    "\n",
    "# Embeddings lookup (subwords -> vectors)\n",
    "with torch.no_grad():\n",
    "    embeddings = model.get_input_embeddings()(tokens[\"input_ids\"])\n",
    "print(embeddings.shape)  # (batch, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ab980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
      "         [ 0.0189, -0.0289, -0.0768,  ...,  0.0116, -0.0212,  0.0171],\n",
      "         [-0.0134, -0.0135,  0.0250,  ...,  0.0013, -0.0183,  0.0227],\n",
      "         [-0.0369, -0.0211, -0.0339,  ..., -0.0305, -0.0492, -0.0583],\n",
      "         [-0.0207, -0.0020, -0.0118,  ...,  0.0128,  0.0200,  0.0259],\n",
      "         [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]]])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d9250",
   "metadata": {},
   "source": [
    "### **Byte-level / character-level** \n",
    "\n",
    "- operates on bytes or characters; smaller vocab, robust to unknown words but longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4bc36",
   "metadata": {},
   "source": [
    "* Byte-level Tokenization\n",
    "\n",
    "Used in `GPT-2/GPT-3/GPT-4` for robustness (no OOV).\n",
    "\n",
    "(Behind the scenes: characters, emojis, and punctuation are split into UTF-8 byte sequences.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1184eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[41762,   364, 12520,   248,   222]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Transformers ðŸš€\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(tokens[\"input_ids\"])  # byte-level tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b5def",
   "metadata": {},
   "source": [
    "* Character-level Tokenization\n",
    "\n",
    "Character-level embeddings treat each character as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f4b6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "# Character-level embeddings treat each character as a token.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Toy character vocabulary\n",
    "chars = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "vocab = {c: i for i, c in enumerate(chars)}\n",
    "embedding_dim = 16\n",
    "\n",
    "# Character embedding matrix\n",
    "char_embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "\n",
    "# Encode a string\n",
    "text = \"data\"\n",
    "ids = torch.tensor([vocab[c] for c in text])  # [3, 0, 19, 0]\n",
    "embeds = char_embedding(ids)\n",
    "print(embeds.shape)  # (len(text), embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286cb5d",
   "metadata": {},
   "source": [
    "### **Hybrid / morpheme-aware** \n",
    "\n",
    "- linguistically informed segmentation for some languages.\n",
    "\n",
    "- Some languages (e.g., Korean, Turkish, Finnish) benefit from morpheme segmentation before embedding. Tools like SentencePiece with Unigram LM or external analyzers are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18536baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Example: train unigram tokenizer\n",
    "spm.SentencePieceTrainer.train(input='data.txt', model_prefix='morph', vocab_size=8000, model_type='unigram')\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=\"morph.model\")\n",
    "text = \"transformers are powerful\"\n",
    "print(sp.encode(text, out_type=int))  # morpheme-level token ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb48947",
   "metadata": {},
   "source": [
    "### **Contextual embeddings (layered)** \n",
    "\n",
    "- initial token embeddings are static vectors; the Transformer produces contextualized vectors after self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0c1a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Projects/Building-LLMs-from-scratch/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings: torch.Size([1, 13, 768])\n",
      "Contextual embeddings (last layer): torch.Size([1, 13, 768])\n",
      "Number of layers (including embedding layer): 13\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load a pretrained Transformer (BERT in this example)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"The bank will not lend money near the river bank.\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens, output_hidden_states=True)\n",
    "\n",
    "# Raw token embeddings (input layer)\n",
    "input_embeddings = outputs.hidden_states[0]   # shape: (batch, seq_len, hidden_dim)\n",
    "\n",
    "# Contextual embeddings from final Transformer layer\n",
    "contextual_embeddings = outputs.last_hidden_state  # shape: (batch, seq_len, hidden_dim)\n",
    "\n",
    "# Contextual embeddings from all layers\n",
    "all_layer_embeddings = outputs.hidden_states  # tuple of length num_layers+1\n",
    "\n",
    "print(\"Input embeddings:\", input_embeddings.shape)\n",
    "print(\"Contextual embeddings (last layer):\", contextual_embeddings.shape)\n",
    "print(\"Number of layers (including embedding layer):\", len(all_layer_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cea84",
   "metadata": {},
   "source": [
    "## **Key properties and desiderata**\n",
    "\n",
    "- `Dimensionality ($D$)` â€” determines representational capacity and parameter cost; larger $D$ often helps scale but increases compute.\n",
    "\n",
    "- `Vocabulary size ($V$)` â€” trade-off: large $V$ reduces tokenization splits but increases embedding parameters; subword tokenizers balance this.\n",
    "\n",
    "- `Sparsity & frequency bias` â€” embeddings reflect training-token frequency; rare tokens can have poorly learned vectors.\n",
    "\n",
    "- `Semantic structure` â€” well-trained embeddings encode lexical similarity, morphology, subword composition.\n",
    "\n",
    "- `Index stability` â€” token ids must be consistent across training/serving; vocabulary changes break embedding alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340dc67",
   "metadata": {},
   "source": [
    "## **Training strategies & initialization**\n",
    "\n",
    "- `Random init then train` â€” most common; embeddings learned end-to-end.\n",
    "\n",
    "- `Pretrained embeddings` â€” initialize from external vectors (word2vec, fastText) then fine-tune; less common for large LMs but useful in low-resource setups.\n",
    "\n",
    "- `Freezing / partial freezing` â€” freeze embedding rows to regularize or save compute; sometimes freeze rare-token embeddings.\n",
    "\n",
    "- `Tying / factorization` â€” tie input and output matrices ($W = E^\\top$) or factorize $E = A B$ (low-rank) to reduce params.\n",
    "\n",
    "- `Adaptive softmax / adaptive input` â€” partition vocabulary into frequency bands and allocate different embedding sizes per band to save parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e0210",
   "metadata": {},
   "source": [
    "## **Regularization & optimization considerations**\n",
    "\n",
    "- `Embedding dropout` â€” drop entire token embeddings (or mask positions) to prevent overfitting.\n",
    "\n",
    "- `Norm constraints` â€” clip embedding norms or apply layer norm after embeddings to stabilize training.\n",
    "\n",
    "- `Label smoothing & temperature` â€” influence gradient signal back to embedding rows through output softmax.\n",
    "\n",
    "- `Learning rate scheduling` â€” embeddings often benefit from same schedules as model, but sometimes different (lower) LR for pretrained init."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1120f",
   "metadata": {},
   "source": [
    "## **Handling rare tokens & OOV**\n",
    "\n",
    "- `Subwords` â€” reduces OOVs by decomposing unknown words into known subwords.\n",
    "\n",
    "- `Byte-level` â€” truly no-OOV (works with arbitrary input) at cost of longer sequences.\n",
    "\n",
    "- `Fallback / UNK token` â€” maps unknowns to a single vector (lossy).\n",
    "\n",
    "- `Compositional approximations` â€” build token vectors from character/subword composition using summation, CNNs, or small encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7972f",
   "metadata": {},
   "source": [
    "## **Compression & efficiency techniques (practical)**\n",
    "\n",
    "- `Parameter sharing / tying` â€” share input & output embeddings.\n",
    "\n",
    "- `Low-rank factorization` â€” represent $E \\approx A B$ with $A\\in\\mathbb{R}^{V\\times r}, B\\in\\mathbb{R}^{r\\times D}`$.\n",
    "\n",
    "- `Quantization` â€” 8-bit/4-bit store and compute with minimal accuracy loss.\n",
    "\n",
    "- `Product quantization / vector quantization` â€” compress embeddings to codebooks.\n",
    "\n",
    "- `Pruning & sparse embeddings` â€” zero-out low-importance entries or use hashed embeddings.\n",
    "\n",
    "- `Adaptive input size` â€” smaller embeddings for rare tokens (adaptive softmax/input)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410677f",
   "metadata": {},
   "source": [
    "## **Output-layer design and weight tying (practical impact)**\n",
    "\n",
    "- `Untied output` â€” separate output projection $W$ allows different geometry between input and output spaces.\n",
    "\n",
    "- `Tied weights` ($W = E^\\top$) â€” reduces parameters and empirically improves perplexity and calibration in many setups (forces input and output geometry alignment).\n",
    "\n",
    "- `Bias term` â€” adding bias $b$ per token helps model token priors (frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1d4b9",
   "metadata": {},
   "source": [
    "## **Diagnostics and probing**\n",
    "\n",
    "- `Nearest-neighbor checks` â€” verify semantically similar tokens are near in embedding space.\n",
    "\n",
    "- `PCA / t-SNE visualization` â€” inspect clusters (POS, subword patterns).\n",
    "\n",
    "- `Frequency vs quality plots` â€” check embedding norm / gradient magnitude vs token frequency.\n",
    "\n",
    "- `Probing tasks` â€” lexical tasks (POS, morphology) to see what lexical info embeddings encode.\n",
    "\n",
    "- `Embedding collapse detection` â€” watch for many embedding rows collapsing to similar vectors (often sign of bad LR or tokenization mismatch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a00c0b",
   "metadata": {},
   "source": [
    "## **Practical guidelines & best practices (concise)**\n",
    "\n",
    "- Use subword tokenization (BPE/WordPiece/Unigram) for most languages; prefer byte-level for robustness to noisy inputs.\n",
    "\n",
    "- Choose embedding dimension $D$ consistent with model size; scale $D$ upward with model depth and attention heads.\n",
    "\n",
    "- Tie weights between input and output for constrained budgets and often better calibration.\n",
    "\n",
    "- For extremely large $V$, adopt adaptive input/softmax or factorization to save memory.\n",
    "\n",
    "- For long-context extrapolation, tokens should remain consistent; positional strategy interacts with tokenization (e.g., byte-level increases sequence length â€” account for positional scheme).\n",
    "\n",
    "- Monitor rare-token gradients and consider upsampling or using compositional encoders if many rare tokens exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8dbcd",
   "metadata": {},
   "source": [
    "## **Modern variations & research directions (short list)**\n",
    "\n",
    "- `Subword regularization / sampling` â€” robustness by sampling alternate tokenizations during training.\n",
    "\n",
    "- `Mixture-of-embeddings` â€” combine multiple embedding sources (lexical + morphological).\n",
    "\n",
    "- `Explicit lexical priors` â€” incorporate POS, lemma, or morphological features into embeddings.\n",
    "\n",
    "- `Cross-lingual shared vocabularies` â€” multilingual models share embeddings across languages; requires careful tokenization and script handling.\n",
    "\n",
    "- `Sparse / retrieval-augmented embeddings` â€” embeddings stored in external index and retrieved when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f70c9",
   "metadata": {},
   "source": [
    "## **Short summary (one-line)**\n",
    "\n",
    "Token embeddings convert discrete tokens to continuous vectors; design choices (tokenizer, $V$, $D$, tying, compression) crucially trade off expressivity, generalization, and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ba83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
