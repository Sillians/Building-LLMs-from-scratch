{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd07366",
   "metadata": {},
   "source": [
    "# **Shortcut (Residual) Connections in Transformers**\n",
    "\n",
    "A shortcut connection (also called a `residual connection`) is a mechanism where the `input` to a layer is added back to the `output` of that layer. In Transformers, this technique ensures that information from earlier layers can flow directly to later layers without being completely transformed at each step.\n",
    "\n",
    "It was first introduced in `ResNet` for `CNNs` and adopted in Transformers to stabilize training and allow deeper architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4b60a",
   "metadata": {},
   "source": [
    "## **Intuition**\n",
    "\n",
    "- Deep networks can suffer from `vanishing gradients` and information loss.\n",
    "\n",
    "- Residual connections act as an `information highway`, ensuring that the original signal (input embeddings, intermediate representations) can `bypass transformations`.\n",
    "\n",
    "- This allows layers to focus on learning `residual transformations` (the “difference” from the input) rather than relearning the full representation.\n",
    "\n",
    "\n",
    "## **Mathematical Representation**\n",
    "\n",
    "Given a layer transformation $F(x, \\theta)$ applied to an input $x$:\n",
    "\n",
    "$$y = F(x, \\theta) + x$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $F(x, \\theta)$ = transformation (e.g., self-attention, feed-forward network).\n",
    "\n",
    "- $x$ = input (shortcut path).\n",
    "\n",
    "- $y$ = output after residual connection.\n",
    "\n",
    "\n",
    "In Transformers (after adding normalization), we typically have:\n",
    "\n",
    "$$y = \\mathrm{LayerNorm}(F(x, \\theta) + x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6955f",
   "metadata": {},
   "source": [
    "## **Key Properties**\n",
    "\n",
    "- `Stabilizes training` of very deep models.\n",
    "\n",
    "- `Prevents vanishing/exploding gradients` by allowing gradient backpropagation through shortcut paths.\n",
    "\n",
    "- `Improves convergence speed` by simplifying optimization.\n",
    "\n",
    "- `Encourages feature reuse` from earlier layers.\n",
    "\n",
    "\n",
    "\n",
    "## **Use Cases**\n",
    "\n",
    "- `Transformers:` used in both encoder and decoder blocks (around attention and feedforward layers).\n",
    "\n",
    "- `ResNets (CNNs):` pioneered the use of residuals for deep convolutional nets.\n",
    "\n",
    "- `Graph Neural Networks (GNNs):` ensure message passing across multiple hops without over-smoothing.\n",
    "\n",
    "- `RNN Variants:` some architectures use residuals to stabilize long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57e881",
   "metadata": {},
   "source": [
    "- **Basic Residual Connection in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abedffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor, sublayer):\n",
    "        \"\"\" \n",
    "        x: input tensor\n",
    "        sublayer: function/layer to apply on x\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b269070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "# Example with feed-forward sublayer\n",
    "ff = nn.Sequential(\n",
    "    nn.Linear(512, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2048, 512)\n",
    ")\n",
    "\n",
    "residual = ResidualConnection(size=512)\n",
    "x = torch.randn(10, 20, 512)  # (batch, seq_len, hidden_dim)\n",
    "output = residual(x, ff)\n",
    "print(output.shape)  # torch.Size([10, 20, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "class ResidualConnection2(nn.Module):\n",
    "    def __init__(self, size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(512, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "res = ResidualConnection2(size=512)\n",
    "x = torch.randn(10, 20, 512) \n",
    "output = res(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309915a5",
   "metadata": {},
   "source": [
    "- **Residual in Transformer Encoder Layer (PyTorch built-in)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0aef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import TransformerEncoderLayer\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "x = torch.randn(10, 32, 512)  # (sequence_len, batch_size, hidden_dim)\n",
    "output = encoder_layer(x)\n",
    "print(output.shape)  # torch.Size([10, 32, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cbda9",
   "metadata": {},
   "source": [
    "- from the preceeding code, `TransformerEncoderLayer` automatically applies `residual + layer norm` around self-attention and feedforward layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b0082",
   "metadata": {},
   "source": [
    "Input x\n",
    "   │\n",
    "[LayerNorm]\n",
    "   │\n",
    "[Self-Attention / FFN] ---> Output\n",
    "   │\n",
    "   +───────────────(Residual)───────────────+\n",
    "   │                                        │\n",
    "Final Output = LayerNorm(Output + Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c0dce",
   "metadata": {},
   "source": [
    "`Shortcut (Residual) connections` are the backbone of `Transformers’` ability to scale deeply while remaining trainable. They allow layers to refine information without destroying it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
