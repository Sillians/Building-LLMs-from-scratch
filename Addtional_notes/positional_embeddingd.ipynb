{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea471d2",
   "metadata": {},
   "source": [
    "# **Positional Embedding in Transformers**\n",
    "\n",
    "`Positional embeddings` are crucial components in Transformer architectures that provide information about the position of tokens in a sequence. Since Transformers process all tokens in parallel without recurrence or convolution, they lack inherent positional awareness, making positional embeddings essential for understanding sequence order.\n",
    "\n",
    "\n",
    "Below are the most widely used positional-encoding families:\n",
    "\n",
    "- `Sinusoidal (fixed) Positional Embedding`\n",
    "  \n",
    "- `Learned (Absolute) Positional Embedding`\n",
    "  \n",
    "- `Relative (Shaw / Transformer-XL / T5 style) Positional Embedding`\n",
    "  \n",
    "- `Rotary (RoPE) Positional Embedding`\n",
    "  \n",
    "- `ALiBi (linear attention bias) Positional Embedding`\n",
    "  \n",
    "- `Convolutional / Local  Positional Embedding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d469626",
   "metadata": {},
   "source": [
    "## **1. Absolute Positional Embedding**\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Absolute positional embeddings assign a fixed, learnable vector to each position in the sequence. Each position index gets its own unique embedding vector. This gives each token a unique, continuous, and interpretable position signal.\n",
    "\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "For a sequence of length L and embedding dimension D:\n",
    "\n",
    "`Even dimensions`\n",
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\tfrac{2i}{D}}}\\right)\n",
    "$$  \n",
    "\n",
    "`Odd dimensions`\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\tfrac{2i}{D}}}\\right)\n",
    "$$ \n",
    "\n",
    "\n",
    "**Key Properties**\n",
    "\n",
    "- Parameter-free, deterministic.\n",
    "\n",
    "- Supports extrapolation to unseen sequence lengths.\n",
    "\n",
    "- Encodes relative positions via linear relationships.\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "- Early Transformers, tasks needing generalization to longer inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9095130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sinusoidal_positional_encoding(max_len, d_model, device=None):\n",
    "    device = device or torch.device(\"cpu\")\n",
    "    pos = torch.arange(max_len, device=device).unsqueeze(1)\n",
    "    i = torch.arange(d_model, device=device).unsqueeze(0)\n",
    "    angle_rates = pos / (10000 ** ( (2 * (i//2)) / d_model ))\n",
    "    pe = torch.zeros(max_len, d_model, device=device)\n",
    "    pe[:, 0::2] = torch.sin(angle_rates[:, 0::2])\n",
    "    pe[:, 1::2] = torch.cos(angle_rates[:, 1::2])\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43101833",
   "metadata": {},
   "source": [
    "## **2. Learned (absolute) Positional Embedding**\n",
    "\n",
    "Each absolute position gets a trainable embedding vector, like word embeddings. Common in models like BERT, it adapts position signals directly from data.\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "$PE_{pos} \\in \\mathbb{R}^{D}$ is learned; input becomes $x_{pos} + PE_{pos}$.\n",
    "\n",
    "\n",
    "**Key Properties**\n",
    "\n",
    "- Flexible, adapts to data distribution.\n",
    "\n",
    "- No natural extrapolation beyond training length.\n",
    "\n",
    "- Simple and widely used.\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "- Large-scale pretraining (e.g., BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da8bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        return self.pos_emb(positions).expand(x.size(0), -1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b295d7",
   "metadata": {},
   "source": [
    "## **3. Relative Positional Encoding**\n",
    "\n",
    "Proposed by Shaw et al. (2018) and extended in Transformer-XL and T5, this focuses on distance between tokens rather than absolute position, improving generalization to variable lengths.\n",
    "\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "$e_{ij} = q_i^\\top k_j + a_{(i-j)}$\n",
    "\n",
    "where $a_{(i-j)}$ is a learned embedding of relative distance.\n",
    "\n",
    "\n",
    "**Key Properties**\n",
    "\n",
    "- Models relative distance directly.\n",
    "  \n",
    "- Improves long-range dependency handling.\n",
    "  \n",
    "- Slightly more complex to implement.\n",
    "\n",
    "\n",
    "**Use cases**\n",
    "\n",
    "- Machine translation, long-sequence modeling (Transformer-XL, T5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9869eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RelativeBias(nn.Module):\n",
    "    def __init__(self, num_heads, max_rel_dist):\n",
    "        super().__init__()\n",
    "        self.relative_bias = nn.Embedding(2 * max_rel_dist + 1, num_heads)\n",
    "        self.max_rel_dist = max_rel_dist\n",
    "\n",
    "    def forward(self, qlen, klen, device=None):\n",
    "        device = device or torch.device(\"cpu\")\n",
    "        ctx = torch.arange(qlen, device=device)[:, None]\n",
    "        mem = torch.arange(klen, device=device)[None, :]\n",
    "        rel_pos = mem - ctx\n",
    "        clipped = rel_pos.clamp(-self.max_rel_dist, self.max_rel_dist) + self.max_rel_dist\n",
    "        bias = self.relative_bias(clipped)\n",
    "        return bias.permute(2, 0, 1)  # [num_heads, qlen, klen]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c6fbe",
   "metadata": {},
   "source": [
    "## **4. Rotary Positional Embedding (RoPE)**\n",
    "\n",
    "Introduced in RoFormer (Su et al., 2021), RoPE applies a rotation to `query/key` vectors based on position, encoding relative information multiplicatively. Popular in modern LLMs (e.g., LLaMA).\n",
    "\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "For pair $(x_{2k}, x_{2k+1})$:\n",
    "\n",
    "$\\begin{bmatrix} x'_{2k} \\\\ x'_{2k+1} \\end{bmatrix} = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix} \\begin{bmatrix} x_{2k} \\\\ x_{2k+1} \\end{bmatrix}$,\n",
    "with $\\theta = pos / 10000^{2k/D}$.\n",
    "\n",
    "\n",
    "**Key Properties**\n",
    "\n",
    "- Relative distance encoded in dot product.\n",
    "\n",
    "- Naturally extrapolates to longer contexts.\n",
    "\n",
    "- Efficient, widely adopted.\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "- Large-scale autoregressive LLMs (e.g., GPTNeoX, LLaMA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d978bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def rotary_angles(seq_len, dim, device=None):\n",
    "    device = device or torch.device(\"cpu\")\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    positions = torch.arange(seq_len, device=device).float()\n",
    "    return positions[:, None] * inv_freq[None, :]\n",
    "\n",
    "def apply_rope(x, angles):\n",
    "    b, n, d = x.shape\n",
    "    x = x.view(b, n, d//2, 2)\n",
    "    cos, sin = torch.cos(angles)[None, :, :, None], torch.sin(angles)[None, :, :, None]\n",
    "    x1 = x[..., 0:1] * cos - x[..., 1:2] * sin\n",
    "    x2 = x[..., 0:1] * sin + x[..., 1:2] * cos\n",
    "    return torch.cat([x1, x2], dim=-1).view(b, n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac13082",
   "metadata": {},
   "source": [
    "## **5. ALiBi (Attention with Linear Biases)**\n",
    "\n",
    "Proposed by Press et al. (2021), `ALiBi` adds a simple linear bias to attention scores based on distance. It encodes recency preference without explicit embeddings, making it scalable to arbitrary lengths.\n",
    "\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "$$bias_{i,j}^{(h)} = -s_h \\cdot (j-i)$$\n",
    "\n",
    "\n",
    "**Key Properties**\n",
    "\n",
    "- Extremely lightweight, parameter-efficient.\n",
    "\n",
    "- Encourages attention to nearby tokens.\n",
    "\n",
    "- Generalizes seamlessly to long sequences.\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "- GPT-like causal LMs with long context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ef065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def get_alibi_slopes(n_heads):\n",
    "    def get_slopes(n):\n",
    "        start = 2.0 ** (-2.0 ** -(math.log2(n) - 3))\n",
    "        return [start * (start ** i) for i in range(n)]\n",
    "    return torch.tensor(get_slopes(n_heads), dtype=torch.float32)\n",
    "\n",
    "def alibi_bias(qlen, klen, slopes, device=None):\n",
    "    device = device or torch.device(\"cpu\")\n",
    "    q_pos = torch.arange(qlen, device=device)[:, None]\n",
    "    k_pos = torch.arange(klen, device=device)[None, :]\n",
    "    rel_dist = (k_pos - q_pos).clamp(min=0)\n",
    "    return -slopes[:, None, None] * rel_dist[None, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78527572",
   "metadata": {},
   "source": [
    "## **6. Convolutional / Local Positional Encodings**\n",
    "\n",
    "Inspired by CNNs, local positional encodings apply convolutional filters over embeddings to introduce local context. This biases the model toward nearby dependencies, useful in `language`, `speech`, and `vision`.\n",
    "\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "$Y = \\mathrm{Conv1D}(X)$ over sequence dimension.\n",
    "\n",
    "\n",
    "**Key Properties**\n",
    "\n",
    "- Strong local inductive bias.\n",
    "\n",
    "- Lightweight and efficient.\n",
    "\n",
    "- Complements global self-attention.\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "- Speech recognition (Conformer), hybrid models (ConvBERT).\n",
    "- Vision Transformers (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237fbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, kernel_size=3, groups=None):\n",
    "        super().__init__()\n",
    "        groups = groups or d_model\n",
    "        self.conv = nn.Conv1d(d_model, d_model, kernel_size,\n",
    "                              padding=(kernel_size-1)//2, groups=groups)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x.transpose(1, 2))\n",
    "        return x + self.act(y.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0fff3",
   "metadata": {},
   "source": [
    "## **Summary (Trade-offs)**\n",
    "\n",
    "- `Sinusoidal` → No parameters, extrapolates well.\n",
    "\n",
    "- `Learned absolute` → Flexible but no extrapolation.\n",
    "\n",
    "- `Relative` → Models distances, better long-sequence handling.\n",
    "\n",
    "- `RoPE` → Encodes relative phase, efficient, widely used in LLMs.\n",
    "\n",
    "- `ALiBi` → Simple linear bias, scalable to very long contexts.\n",
    "\n",
    "- `Convolutional` → Local bias, strong for speech/vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
