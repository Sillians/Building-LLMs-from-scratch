{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a06d2b",
   "metadata": {},
   "source": [
    "# **Dropout in Transformers and Deep Learning**\n",
    "\n",
    "- `Dropout` is a `regularization technique` where a fraction of `neurons` (activations) are randomly “dropped” (set to zero) during training.\n",
    "\n",
    "- In Transformers and other deep learning models, dropout helps prevent `overfitting`, improves generalization, and stabilizes training.\n",
    "\n",
    "- At inference, dropout is disabled, and the full network is used with scaled weights.\n",
    "\n",
    "\n",
    "### **Intuition**\n",
    "\n",
    "- Deep networks often `co-adapt`: neurons rely too heavily on specific patterns.\n",
    "\n",
    "- Dropout forces the network to `not depend on any single neuron`, making it more `robust`.\n",
    "\n",
    "- Think of it as a `“neural committee”`: each forward pass trains a slightly different subnet, and at inference, their ensemble effect is approximated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e4611",
   "metadata": {},
   "source": [
    "### **Mathematical Representation**\n",
    "\n",
    "Let $h = f(x, W)$ be the output of a hidden layer.\n",
    "Dropout introduces a random binary mask $m \\sim \\mathrm{Bernoulli}(p)$, where $p$ is the keep probability (opposite of dropout rate).\n",
    "\n",
    "$$h' = \\frac{m \\odot h}{p}$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\odot$ = elementwise multiplication\n",
    "\n",
    "- $p$ = probability of keeping a neuron (e.g., $p=0.9$ for 10% dropout)\n",
    "\n",
    "- Scaling by $1/p$ ensures expectation remains unchanged: $E[h’] = h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropout in Transformers**\n",
    "\n",
    "Transformers use dropout in several places:\n",
    "\n",
    "**1. Attention Weights**\n",
    "\n",
    "- After computing attention scores (softmax), dropout is applied to randomly drop attention links.\n",
    "\n",
    "$\\mathrm{Attention}(Q,K,V) = \\mathrm{Dropout}(\\mathrm{Softmax}(QK^T / \\sqrt{d_k})) V$\n",
    "\n",
    "\n",
    "**2. Feedforward Layers**\n",
    "\n",
    "- Dropout is applied after non-linear activations to prevent overfitting.\n",
    "\n",
    "\n",
    "**3. Residual Connections**\n",
    "\n",
    "- Dropout is applied before adding residuals to stabilize training.\n",
    "\n",
    "\n",
    "**4. Embedding Layers**\n",
    "\n",
    "- Sometimes applied to word/token embeddings for additional regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396bdba",
   "metadata": {},
   "source": [
    "### **Key Properties**\n",
    "\n",
    "\n",
    "- `Regularization:` prevents overfitting on small/medium datasets.\n",
    "\n",
    "\n",
    "- `Stochastic Training:` makes model robust to input noise.\n",
    "\n",
    "\n",
    "- `Improved Generalization:` encourages distributed representations.\n",
    "\n",
    "\n",
    "- `Scalable:` effective in both small MLPs and very deep models like Transformers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Use Cases**\n",
    "\n",
    "- `Deep Learning:` CNNs, RNNs, MLPs to combat overfitting.\n",
    "\n",
    "\n",
    "- `Transformers:` GPT, BERT, Vision Transformers (ViT) use dropout for regularization.\n",
    "\n",
    "\n",
    "- `Large-Scale Models:` often combined with other regularizers (weight decay, layer norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2d80a",
   "metadata": {},
   "source": [
    "- **Basic Dropout in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3453bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: tensor([[ 1.1765, -0.6996,  1.4068,  1.8871, -2.0013,  0.2409, -1.1512, -0.0000,\n",
      "         -0.5173, -0.8496],\n",
      "        [-0.0000,  1.2326,  1.2722, -2.1774, -0.9650,  3.0767,  0.0000, -0.0000,\n",
      "          0.7171, -1.8683],\n",
      "        [ 0.0000,  0.7751, -0.0000,  0.5976, -0.0000,  0.4695, -0.0000, -0.0000,\n",
      "         -1.4724,  0.2547],\n",
      "        [ 0.0000, -0.1214, -0.3874, -1.2169,  0.0000,  0.7657,  0.5046,  0.0000,\n",
      "         -0.0315, -2.2355],\n",
      "        [ 0.3853, -1.8736,  0.0000, -1.1200, -0.0000, -0.0423,  0.0000, -0.3152,\n",
      "         -0.0000, -0.1550]])\n",
      "Inference: tensor([[ 0.8236, -0.4897,  0.9848,  1.3209, -1.4009,  0.1687, -0.8058, -0.3217,\n",
      "         -0.3621, -0.5947],\n",
      "        [-1.2429,  0.8628,  0.8905, -1.5242, -0.6755,  2.1537,  1.8196, -0.1857,\n",
      "          0.5020, -1.3078],\n",
      "        [ 0.6190,  0.5426, -0.2903,  0.4183, -0.5684,  0.3287, -0.5043, -1.4682,\n",
      "         -1.0307,  0.1783],\n",
      "        [ 0.7345, -0.0850, -0.2712, -0.8519,  2.1344,  0.5360,  0.3533,  0.7857,\n",
      "         -0.0221, -1.5649],\n",
      "        [ 0.2697, -1.3115,  0.6802, -0.7840, -0.0035, -0.0296,  0.2051, -0.2207,\n",
      "         -1.8340, -0.1085]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn(5, 10)\n",
    "dropout = nn.Dropout(p=0.3) # Drop 30% of neurons\n",
    "\n",
    "dropout.train()\n",
    "print(\"Training:\", dropout(x)) # Some neurons are zeroed\n",
    "\n",
    "dropout.eval()\n",
    "print(\"Inference:\", dropout(x)) # No dropout applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1557250c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8236, -0.4897,  0.9848,  1.3209, -1.4009,  0.1687, -0.8058, -0.3217,\n",
       "         -0.3621, -0.5947],\n",
       "        [-1.2429,  0.8628,  0.8905, -1.5242, -0.6755,  2.1537,  1.8196, -0.1857,\n",
       "          0.5020, -1.3078],\n",
       "        [ 0.6190,  0.5426, -0.2903,  0.4183, -0.5684,  0.3287, -0.5043, -1.4682,\n",
       "         -1.0307,  0.1783],\n",
       "        [ 0.7345, -0.0850, -0.2712, -0.8519,  2.1344,  0.5360,  0.3533,  0.7857,\n",
       "         -0.0221, -1.5649],\n",
       "        [ 0.2697, -1.3115,  0.6802, -0.7840, -0.0035, -0.0296,  0.2051, -0.2207,\n",
       "         -1.8340, -0.1085]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bfd2c1",
   "metadata": {},
   "source": [
    "- **Dropout in a Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc9df66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout), # Dropout in feedforward\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # Self-attention + dropout + residual\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feedforward + dropout + residual\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout2(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd774604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Basic Usage ===\n",
      "Input shape: torch.Size([4, 10, 512])\n",
      "Output shape: torch.Size([4, 10, 512])\n",
      "Input and output shapes match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic Usage\n",
    "def basic_usage_example():\n",
    "    # Hyperparameters\n",
    "    batch_size = 4\n",
    "    seq_length = 10\n",
    "    embed_dim = 512\n",
    "    num_heads = 8\n",
    "    ff_dim = 2048\n",
    "    \n",
    "    # create transformer block\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    \n",
    "    # Create sample input (batch_size, seq_length, embed_dim)\n",
    "    x = torch.randn(batch_size, seq_length, embed_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = transformer_block(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Input and output shapes match: {x.shape == output.shape}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"===Basic Usage ===\")\n",
    "output1 = basic_usage_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d5d115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8910, -0.5180, -0.8523,  ...,  0.2387,  0.5434,  1.1064],\n",
       "         [ 0.3783, -1.9540,  0.4625,  ..., -0.5516, -1.7609, -0.1673],\n",
       "         [-2.9832,  0.4322,  1.1139,  ...,  0.0755,  1.3698, -0.6721],\n",
       "         ...,\n",
       "         [ 0.3106,  0.2099, -0.4223,  ..., -0.5154,  0.2615,  0.5391],\n",
       "         [ 0.9403,  1.2124, -1.3579,  ..., -1.0018,  0.0729,  0.2915],\n",
       "         [-0.6229, -0.4428, -0.7226,  ...,  0.3517, -0.7761,  0.2363]],\n",
       "\n",
       "        [[ 0.1852,  1.2087,  0.0623,  ...,  0.0879, -0.4299, -0.6647],\n",
       "         [ 0.0890, -1.5246, -0.2792,  ...,  0.8312,  0.2728, -0.8954],\n",
       "         [-1.5291,  0.4701, -1.2672,  ...,  0.3608,  0.0942,  0.4916],\n",
       "         ...,\n",
       "         [ 0.2830, -1.4619, -1.5875,  ..., -1.0349, -1.3666, -1.3967],\n",
       "         [ 0.4464, -0.4948, -0.3675,  ...,  0.0912, -0.9250,  1.1598],\n",
       "         [-0.7500,  0.1630,  0.8844,  ..., -0.7052,  0.5838,  1.4363]],\n",
       "\n",
       "        [[-1.0349, -1.9986,  0.0330,  ..., -1.1214, -0.0614, -0.7261],\n",
       "         [ 2.0492,  0.3298, -1.0137,  ..., -0.8493,  0.5821, -1.1289],\n",
       "         [ 1.2589, -0.3445,  0.9955,  ..., -0.6271,  1.2804,  2.2748],\n",
       "         ...,\n",
       "         [ 1.0503,  1.0792, -0.1188,  ...,  0.5173, -0.5913,  0.7261],\n",
       "         [ 0.9716,  0.6698, -1.0703,  ...,  0.7968,  2.9112,  0.5237],\n",
       "         [-0.4295,  1.3570, -1.3937,  ...,  1.7345,  0.1695, -0.9629]],\n",
       "\n",
       "        [[-1.3186,  0.2406,  0.3076,  ..., -0.9304,  0.8056,  1.2755],\n",
       "         [ 0.5080,  1.6236,  0.4166,  ..., -0.9125, -2.2032,  0.4147],\n",
       "         [-0.0914,  0.6830,  0.6149,  ...,  2.0986,  0.1140, -0.4969],\n",
       "         ...,\n",
       "         [ 0.7959,  1.0246,  0.4422,  ...,  1.4264,  0.7487,  0.1762],\n",
       "         [ 0.4766,  1.8337,  0.2043,  ...,  1.9694, -0.7762, -0.8438],\n",
       "         [-2.1273,  0.1490,  1.2280,  ..., -0.6886, -1.0049, -1.6615]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831748ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 2: Multiple Blocks ===\n",
      "Processed through 3 transformer blocks\n",
      "Output shape: torch.Size([2, 8, 256])\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Stacking Multiple Transformer Blocks\n",
    "def multi_block_example():\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "    ff_dim = 1024\n",
    "    num_blocks = 3\n",
    "    \n",
    "    # Create a small transformer with multiple blocks\n",
    "    transformer_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_blocks)]\n",
    "    )\n",
    "    \n",
    "    # Sample input\n",
    "    x = torch.randn(2, 8, embed_dim) # (batch_size, seq_length, embed_dim)\n",
    "    \n",
    "    # Process through all blocks\n",
    "    output = transformer_blocks(x)\n",
    "    \n",
    "    print(f\"Processed through {num_blocks} transformer blocks\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"=== Example 2: Multiple Blocks ===\")\n",
    "output2 = multi_block_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf277ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7984,  0.3157,  1.9364,  ..., -0.3314, -0.5696, -0.9131],\n",
       "         [-0.3177,  1.5079, -0.5233,  ..., -0.1358,  0.5519, -0.6968],\n",
       "         [-0.5598, -1.8233,  0.7427,  ...,  0.5237,  0.5401, -1.5678],\n",
       "         ...,\n",
       "         [ 0.6121,  0.8328, -0.8884,  ...,  1.8705,  0.5036, -0.9058],\n",
       "         [-0.6972,  1.1989,  0.5254,  ...,  0.7133, -0.5159, -1.3623],\n",
       "         [-0.3520, -0.1714,  0.9600,  ..., -0.2660, -0.5049, -1.3907]],\n",
       "\n",
       "        [[ 0.6494,  0.8556, -0.0379,  ...,  0.3544, -0.1506, -0.3147],\n",
       "         [ 0.2344,  1.0797,  0.4952,  ...,  1.6897, -1.2753, -0.5030],\n",
       "         [ 0.7656,  0.9892,  0.9193,  ..., -0.4112,  0.2634,  1.4715],\n",
       "         ...,\n",
       "         [-1.0516, -0.5733,  1.2113,  ...,  0.8083, -0.7357, -0.0677],\n",
       "         [ 0.9197, -0.0543, -1.1508,  ...,  1.4371,  1.8451,  0.9517],\n",
       "         [-0.4894, -0.2065,  1.1197,  ...,  0.1748,  0.3715, -0.6791]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cbe91d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example 3: Training Example ===\n",
      "Model output shape: torch.Size([8, 5])\n",
      "Labels shape: torch.Size([8])\n",
      "Training loss: 1.7553\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Example 3: Training a simple model\n",
    "class SimpleTransformerClassifier(nn.Module):\n",
    "   def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_classes, num_blocks=2):\n",
    "      super().__init__()\n",
    "      self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "      self.transformer_blocks = nn.Sequential(\n",
    "         *[TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_blocks)]\n",
    "      )\n",
    "      self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "      \n",
    "   \n",
    "   def forward(self, x:torch.Tensor):\n",
    "      x = self.embedding(x) # (batch_size, seq_length, embed_dim)\n",
    "      x = self.transformer_blocks(x)\n",
    "      # Use the output of the first token for classification\n",
    "      x = x[:, 0, :]\n",
    "      return self.classifier(x)\n",
    "   \n",
    "\n",
    "def training_example():\n",
    "   # Model parameters\n",
    "    vocab_size = 1000\n",
    "    embed_dim = 128\n",
    "    num_heads = 4\n",
    "    ff_dim = 512\n",
    "    num_classes = 5\n",
    "    seq_length = 20\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    # create model\n",
    "    model = SimpleTransformerClassifier(vocab_size, \n",
    "                                        embed_dim, \n",
    "                                        num_heads, \n",
    "                                        ff_dim, \n",
    "                                        num_classes)\n",
    "    \n",
    "    # Sample data\n",
    "    batch_size = 8\n",
    "    input_seq = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    labels = torch.randint(0, num_classes, (batch_size,))\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_seq)\n",
    "    print(f\"Model output shape: {outputs.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Training setup (simplified)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training step\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Training loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model, loss\n",
    "    \n",
    "\n",
    "print(\"\\n=== Example 3: Training Example ===\")\n",
    "model, loss = training_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0fb9943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleTransformerClassifier(\n",
      "  (embedding): Embedding(1000, 128)\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e680886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example 4: Masked Attention ===\n",
      "Casual mask shape: tensor([[0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "Masked output shape: torch.Size([2, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "# Masked Self-Attention (for language modeling)\n",
    "class MaskedTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, \n",
    "                                          num_heads, \n",
    "                                          dropout=dropout, \n",
    "                                          batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        # Masked self-attention\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask = mask)\n",
    "        x = x + self.dropout1(attn_out) # dropout + residual\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feedforward\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout2(ff_out)  # dropout + residual\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def masked_attention_example():\n",
    "    embed_dim = 64\n",
    "    num_heads = 2\n",
    "    ff_dim = 128\n",
    "    seq_length = 6\n",
    "    \n",
    "    # create casual mask for autoregressive generation\n",
    "    casual_mask = torch.triu(torch.ones(seq_length, seq_length) * float('-inf'), diagonal=1)\n",
    "    \n",
    "    block = MaskedTransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = torch.randn(2, seq_length, 64) # (batch_size, seq_length, embed_dim)\n",
    "    \n",
    "    output = block(x, casual_mask)\n",
    "    print(f\"Casual mask shape: {casual_mask}\")\n",
    "    print(f\"Masked output shape: {output.shape}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"\\n=== Example 4: Masked Attention ===\")\n",
    "output4 = masked_attention_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc671a",
   "metadata": {},
   "source": [
    "**Key Points to Note:**\n",
    "\n",
    "- `Input/Output Shapes:` The `TransformerBlock` maintains the same shape `(batch_size, seq_length, embed_dim)`.\n",
    "\n",
    "- `Residual Connections:` The `x + dropout(layer(x))` pattern preserves information flow.\n",
    "\n",
    "- `Layer Normalization:` Applied after the residual connection (post-norm architecture).\n",
    "\n",
    "- `Batch First:` Note the use of `batch_first=True` in `MultiheadAttention` for consistent tensor shapes.\n",
    "\n",
    "- `Flexibility:` This block can be stacked to create deeper transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1ed35",
   "metadata": {},
   "source": [
    "- **Dropout in Hugging Face Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aeb3f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(model.config.hidden_dropout_prob)  # Dropout rate (e.g., 0.1)\n",
    "print(model.config.attention_probs_dropout_prob)  # Dropout in attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc3f65f",
   "metadata": {},
   "source": [
    "**Practical Notes**\n",
    "\n",
    "- Typical dropout rates in Transformers: `0.1 – 0.3`.\n",
    "\n",
    "- For very large models `(GPT-3, LLaMA)` trained on huge datasets, dropout is sometimes `reduced` or `removed` since data scale itself regularizes training.\n",
    "\n",
    "- Dropout works best when combined with other techniques: `weight decay`, `early stopping`, `label smoothing`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f40749",
   "metadata": {},
   "source": [
    "**Common Use Cases:**\n",
    "\n",
    "- `Text Classification:` Stack multiple blocks + classification head\n",
    "\n",
    "- `Language Modeling:` Use with causal masking\n",
    "\n",
    "- `Sequence-to-Sequence:` Use as `encoder/decoder` blocks\n",
    "\n",
    "- `Feature Extraction:` Extract representations for downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edd932",
   "metadata": {},
   "source": [
    "### **Summary**\n",
    "\n",
    "- `Dropout` = stochastic regularization to improve `generalization`.\n",
    "\n",
    "- In Transformers, it is `woven into attention`, `feedforward`, `residual`, and `embedding layers`.\n",
    "\n",
    "- Crucial for smaller models and moderate datasets, less critical but still used in very large LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
