{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d04e0b",
   "metadata": {},
   "source": [
    "# **Fine-Tuning in LLMs and Multimodal LLMs**\n",
    "\n",
    "\n",
    "## **Definition:**\n",
    "\n",
    "- `Fine-tuning` is the process of adapting a pretrained large model (LLM or multimodal LLM) to a downstream task or domain by updating all or part of its parameters.\n",
    "\n",
    "- Instead of training from scratch (which is computationally prohibitive), `fine-tuning` leverages `general knowledge already learned` and aligns it to `task-specific distributions` (`text classification`, `question answering`, `image-captioning`, `speech-text alignment`, etc.).\n",
    "\n",
    "\n",
    "\n",
    "## **Intuition**\n",
    "\n",
    "- Pretrained LLMs capture broad world knowledge but lack `task specialization`.\n",
    "\n",
    "- Fine-tuning bridges this gap by teaching the model how to `“speak the dialect”` of the `downstream task`.\n",
    "\n",
    "- In multimodal LLMs, fine-tuning aligns `cross-modal representations` (e.g., mapping image embeddings to the same semantic space as text embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ca4dc",
   "metadata": {},
   "source": [
    "## **Mathematical Representation**\n",
    "\n",
    "Let:\n",
    "\n",
    "- $X$ = input data (text, image, audio, etc.)\n",
    "\n",
    "- $f_{\\theta}$ = pretrained LLM with parameters $\\theta$\n",
    "\n",
    "- $\\mathcal{L}$ = task-specific loss (e.g., cross-entropy for classification)\n",
    "\n",
    "\n",
    "Fine-tuning optimizes:\n",
    "\n",
    "$$\\theta^{*} = \\arg \\min_{\\theta} \\; \\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(f_{\\theta}(x), y) \\big]$$\n",
    "\n",
    "\n",
    "Where $\\mathcal{D}$ is the downstream dataset.\n",
    "\n",
    "In practice, $\\theta$ may be:\n",
    "\n",
    "- Fully updated (full fine-tuning)\n",
    "\n",
    "- Partially updated (parameter-efficient fine-tuning, e.g., LoRA, adapters, prefix-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3588d",
   "metadata": {},
   "source": [
    "## **Types of LLM Fine-Tuning**\n",
    "\n",
    "| Method | Definition | Key Properties | Use Cases |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Full Fine-Tuning** | Update **all** of the model's weights using a downstream dataset. | High accuracy but **very computationally costly** (high GPU/TPU memory and time). | Small models, highly specialized domains where performance is critical. |\n",
    "| **Feature Extraction** | Freeze the entire LLM and use its hidden representations (embeddings) as input for an external classifier or model. | Fast, cheap, but less flexible as the LLM itself is not adapted. | Embedding extraction, retrieval-augmented generation (RAG), simple classification tasks. |\n",
    "| **Adapters** | Insert small, trainable layers between the frozen layers of the pre-trained Transformer blocks. | Parameter-efficient, modular (can stack or switch adapters). | Domain adaptation, multi-task learning. |\n",
    "| **LoRA (Low-Rank Adaptation)** | Learn and apply low-rank matrix decompositions to update the attention and linear layers, instead of updating the full weights. | Highly **memory-efficient**, performance often matches full fine-tuning, widely used. | The standard method for fine-tuning large LLMs on small-to-medium scale tasks. |\n",
    "| **Prefix/Prompt-Tuning** | Optimize a small set of continuous, task-specific vectors (a \"soft prompt\") prepended to the input. | **Extremely parameter-efficient**, only a tiny fraction of parameters are trained. | Few-shot learning, quick domain adaptation without changing the model. |\n",
    "| **Instruction-Tuning** | Fine-tune the LLM on a dataset of tasks formatted as **natural language instructions** and desired responses. | Doesn't teach new knowledge but dramatically improves **task generalization** and response formatting. | Training chatbots, models like ChatGPT, and any model that needs to follow user intent. |\n",
    "| **RLHF (Reinforcement Learning from Human Feedback)** | Use reinforcement learning (e.g., PPO) to fine-tune the model based on a reward model trained on **human preferences**. | **Aligns** model outputs with human values and intentions (helpfulness, honesty, harmlessness). | Creating advanced AI assistants like ChatGPT, Claude, and Bard. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db1d14",
   "metadata": {},
   "source": [
    "## **Fine-Tuning in Multimodal LLMs**\n",
    "\n",
    "`Multimodal LLM` fine-tuning extends adaptation across `modalities` (text + vision, text + audio, etc.), ensuring representations from different domains align in a shared semantic space.\n",
    "\n",
    "\n",
    "**Key Approaches:**\n",
    "\n",
    "- `Cross-modal pretraining + fine-tuning:` Align embeddings (e.g., CLIP style, then fine-tune on downstream task).\n",
    "\n",
    "- `Adapter Fusion:` Train adapters for each modality, then fuse for multimodal reasoning.\n",
    "\n",
    "- `LoRA in multimodal layers:` Apply LoRA to vision encoders, audio encoders, and text decoders.\n",
    "\n",
    "- `Instruction-Tuning Multimodal LLMs:` Using multimodal prompts (e.g., “Describe this image…”) for alignment.\n",
    "\n",
    "\n",
    "**Mathematical Sketch**\n",
    "\n",
    "For multimodal input $(x_{text}, x_{vision})$:\n",
    "\n",
    "$$h_{text} = f_{\\theta_{T}}(x_{text}), \\quad h_{vision} = g_{\\theta_{V}}(x_{vision})$$\n",
    "\n",
    "$$z = \\phi(h_{text}, h_{vision})$$\n",
    "\n",
    "\n",
    "Fine-tuning optimizes task loss:\n",
    "\n",
    "$$\\theta^{*} = \\arg \\min_{\\theta} \\; \\mathcal{L}(\\phi(h_{text}, h_{vision}), y)$$\n",
    "\n",
    "Where $\\phi$ is a fusion mechanism (cross-attention, concatenation, projection).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15584d92",
   "metadata": {},
   "source": [
    "## **Use Cases**\n",
    "\n",
    "**LLMs:**\n",
    "\n",
    "- Sentiment analysis, classification, summarization.\n",
    "\n",
    "- Chatbots and domain-specific assistants.\n",
    "\n",
    "- Code generation (finetuning LLMs on code corpora).\n",
    "\n",
    "\n",
    "**Multimodal LLMs:**\n",
    "\n",
    "- Image captioning (COCO, Flickr30k datasets)\n",
    "\n",
    "- Visual question answering (VQA)\n",
    "\n",
    "- Speech-to-text alignment\n",
    "\n",
    "- Multimodal search & retrieval\n",
    "\n",
    "- Robotics (aligning vision + language for action planning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facc41c",
   "metadata": {},
   "source": [
    "## **Code Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160fd76",
   "metadata": {},
   "source": [
    "- **Full Fine-Tuning (Hugging Face LLM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9abe122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Sample dataset\n",
    "texts = [\"I love this!\", \"I hate it.\"]\n",
    "labels = [1, 0]\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "import torch\n",
    "dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], torch.tensor(labels))\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(output_dir=\"./results\", num_train_epochs=2, per_device_train_batch_size=2)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=dataset)\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a8243",
   "metadata": {},
   "source": [
    "- **LoRA Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8b645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "config = LoraConfig(r=8, \n",
    "                    lora_alpha=32, \n",
    "                    lora_dropout=0.1, \n",
    "                    task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Only a few million params trainable\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50c1fa",
   "metadata": {},
   "source": [
    "- **Multimodal Fine-Tuning (CLIP-style)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c634d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image = Image.open(\"dog.png\")\n",
    "text = [\"a photo of a dog\", \"a photo of a cat\"]\n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Similarity scores\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fcbe5d",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "\n",
    "- Fine-tuning in `LLMs` specializes pretrained models for downstream NLP tasks.\n",
    "\n",
    "- Fine-tuning in `Multimodal LLMs` additionally requires aligning heterogeneous modalities in a shared embedding space.\n",
    "\n",
    "- Techniques range from full `fine-tuning` (expensive) to `parameter-efficient methods` (LoRA, adapters, prefix-tuning), with multimodal extensions for cross-domain reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2aacaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
